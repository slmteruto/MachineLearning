{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "\n",
    "    http://hunkim.github.io/ml/\n",
    "    \n",
    "    \n",
    "    CNN : 하나를 여러개로 분리해서 마지막에 다시 합치는 방식 CNN의 알고리즘\n",
    "            현재 데이터를 분석하는 하나의 알고리즘일 뿐이다.\n",
    "            \n",
    "    RNN : 단계별로 그전에 처리됐던 결과를 또 조합시켜서 계산하고 또 이 결과를\n",
    "          다른 레이어에 조합시켜서 이걸 계속 반복시켜 버림\n",
    "          \n",
    "          I google at work\n",
    "          \n",
    "          I work at google\n",
    "          \n",
    "          같은 google인데 위에선 동사고 밑에선 명사이다\n",
    "          CNN은 이걸 구별 못한다.\n",
    "          RNN은 위치에 따른 품사 구별을 할 수 있게 된다. 앞뒤 단어에 따라 \n",
    "          \n",
    "          그전에 뭐가 입력되었는지 알아야하기 때문에 RNN을 쓰게 되면 좋다 \n",
    "          \n",
    "          \n",
    "    \n",
    "        주로 사용하는곳\n",
    "        \n",
    "        음성인식\n",
    "        번역\n",
    "        질문, 대화\n",
    "        \n",
    "        \n",
    "        \n",
    "    강의슬라이드\n",
    "\n",
    "        알고리즘 그림이 있음\n",
    "\n",
    "        원랜 이걸 하나만 썼는데 여러가지를 쓰게 되는 알고리즘이다.\n",
    "\n",
    "        그래서 그전에 어떤값이 들어왔는지를 고려할 수 있게 된 것이다. \n",
    "\n",
    "        상단값  맨위 - 다항분류\n",
    "        state값 중간 - 이진분류, 감정분류\n",
    "\n",
    "    \n",
    "    \n",
    "    실습 슬라이드\n",
    "        hihello를 뽑는 실습        \n",
    "        여러 단어를 분석해야하기 때문에 softmax를 사용\n",
    "        \n",
    "        \n",
    "        Sentiment Analysis  - 감정분석\n",
    "        감정을 뽑아 내서 분석\n",
    "        \n",
    "    \n",
    "    히든 계층이 어떻게 될까\n",
    "            \n",
    "        \n",
    "             y1                      y2\n",
    "             \n",
    "             ↓                      ↓   \n",
    "       -------------            -------------\n",
    "      |             |          |             |\n",
    "      |             |          |             | \n",
    "      |      h1     |   --->   |      h2     |   ---> \n",
    "      |             |          |             |\n",
    "      |      가중치w|          |      가중치w|\n",
    "      |     입력값 x|          |     입력값 o|\n",
    "       --------------           --------------\n",
    "            ↑                        ↑\n",
    "            x1                        x2\n",
    "            \n",
    "            \n",
    "        * 초기 입력값은 없다. \n",
    "            두번째 계산 부터는 전에 계산되었던 값이 넘어와서 영향을 미치게 된다. \n",
    "            \n",
    "            \n",
    "    CNN 의\n",
    "    \n",
    "    RNN 의 Back propergation 이 어떻게 이루어지나\n",
    "        값은 하난데 가능한가?\n",
    "            -> 시간에 따라 BP를 한다. \n",
    "            \n",
    "        \n",
    "        \n",
    "### RNN의 기본 알고리즘\n",
    "    \n",
    "    Vanila RNN 이라 한다.  - 위에서 설명한 것\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 실습 (Vanila RNN)\n",
    "\n",
    "        그냥 데이터를 입력했을 때 결과가 어떻게 나오는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.01837398  0.4150799   0.26174298]\n",
      "  [ 0.39704606 -0.29351166  0.3648948 ]\n",
      "  [-0.45316276 -0.40126407  0.33250588]\n",
      "  [-0.3790015  -0.73852766  0.64487135]\n",
      "  [-0.10647471 -0.20027146  0.31252015]]]\n",
      "---------------\n",
      "[[-0.10647471 -0.20027146  0.31252015]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# 입력값 준비 : hello\n",
    "\n",
    "# 값을 숫자로 바꿔야함 \n",
    "\n",
    "h = [1,0,0,0]  \n",
    "e = [0,1,0,0]\n",
    "l = [0,0,1,0]\n",
    "o = [0,0,0,1]\n",
    "    # 왜 4개?  중복된것을 빼고 네개만 한다. \n",
    "    \n",
    "\n",
    "# cell 준비 (히든계층?)\n",
    "# LSTMCell은 나중에 하고\n",
    "# BasicRNNCell\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=3)\n",
    "                                # 히든 개수를 적어준다. 이걸 수정하면 출력의 갯수도 적은 수만큼 나온다.\n",
    "\n",
    "# 입력 데이터 준비\n",
    "x_data = np.array([[h,e,l,l,o]], dtype=np.float32)  \n",
    "    # 시퀀스가 5개 되게 한다. 그 다음 글자가 무엇이 나올지 예측\n",
    "    # 문자열이기 때문에 입력값이 최대 3차원이 되어야 한다. \n",
    "    # 그 전에 CNN할 땐 이미지여서 4차원으로 하였다. \n",
    "# print(x_data)\n",
    "\n",
    "\n",
    "\n",
    "# RNN 설정\n",
    "output, _state = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    # 얘는 결과가 두개다\n",
    "    # 한개는 출력\n",
    "    # 한개는 옆으로 가는 state\n",
    "\n",
    "    \n",
    "# 세션 준비\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    # 항상 초기화 할것\n",
    "    \n",
    "print(sess.run(output))\n",
    "print(\"---------------\")\n",
    "print(sess.run(_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    위 결과차례대로 h, e, l, l, o 값이고\n",
    "    \n",
    "    num_units = 3으로 적었기 때문에 3열로 값이 나온다. \n",
    "    \n",
    "    마지막 output 값과\n",
    "    마지막 state 값은 같아야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.20793891  0.24406303 -0.75278705]\n",
      "  [-0.06346128 -0.52844936  0.68356085]\n",
      "  [-0.36491966  0.8857268  -0.02324395]]\n",
      "\n",
      " [[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.30707452  0.62735885  0.21719742]\n",
      "  [ 0.5043804  -0.14038289  0.3744523 ]\n",
      "  [-0.11641277  0.70696247 -0.7512605 ]]]\n",
      "---------------\n",
      "[[-0.36491966  0.8857268  -0.02324395]\n",
      " [-0.11641277  0.70696247 -0.7512605 ]]\n",
      "---------------\n",
      "<tf.Variable 'rnn/basic_rnn_cell/kernel:0' shape=(7, 3) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_rnn_cell/bias:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "\n",
    "# I work at google\n",
    "\n",
    "# I google at work\n",
    "\n",
    "### one-hot encoding\n",
    "# i work at google  = [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]\n",
    "# i google at work  = [[1,0,0,0], [0,0,0,1], [0,0,1,0], [0,1,0,0]]\n",
    "\n",
    "# 3차원으로 데이터 준비  (위 두 문장)\n",
    "inputs = np.array([[[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]], \n",
    "                 [[1,0,0,0], [0,0,0,1], [0,0,1,0], [0,1,0,0]]])\n",
    "\n",
    "\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "\n",
    "# cell준비 (기본cell)\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=3)\n",
    "\n",
    "# 실행\n",
    "output, _state = tf.nn.dynamic_rnn(cell, tf_inputs, dtype=tf.float32)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    # 항상 초기화 할것\n",
    "    \n",
    "print(sess.run(output))\n",
    "print(\"---------------\")\n",
    "print(sess.run(_state))\n",
    "print(\"---------------\")\n",
    "# 각 W값 확인\n",
    "variable_names = [v.name for v in tf.trainable_variables()]\n",
    "for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    우리가 한건 훈련이나 그런건 아니다.\n",
    "    입력했을 때 어떤 값이 나오는지를 확인만 한것 뿐이다.\n",
    "    \n",
    "    맨 밑의 값 두개는 \n",
    "    \n",
    "    W, b의 값들이다\n",
    "    W : 갯수가 들어간다. \n",
    "    b : num_units=3 이라서 3개이다.\n",
    "    \n",
    "    \n",
    "    \n",
    "    지금은 우리가 짧은 단어로 했기 때문에 잘 나오는데\n",
    "    데이터가 커지면 바닐라 RNN으로 처리하기 어렵다\n",
    "    \n",
    "    그래서 복잡한 데이터를 하기 위해 나온 알고리즘이 LSTM이다. \n",
    "    \n",
    "    LSTM (Long?)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hihell 입력시  (실습 슬라이드)\n",
    "\n",
    "    hihell 입력시 출력을 ihello를 출력할 수 있게\n",
    "\n",
    "    즉 \n",
    "    i입력 -> h 예측\n",
    "    h -> e 예측\n",
    "    e -> l 예측  이런식이다. \n",
    "    \n",
    "    \n",
    "    hihello 학습 과정\n",
    "        - 중복된 문자 제외 하고 one_hot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 1.7141227      prediction :  [[3 3 3 3 3 3]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "1 loss 1.4223528      prediction :  [[3 3 3 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "2 loss 1.2504609      prediction :  [[2 0 3 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "3 loss 1.0714287      prediction :  [[2 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "4 loss 0.9058055      prediction :  [[2 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "5 loss 0.73478097      prediction :  [[2 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "6 loss 0.58669895      prediction :  [[2 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "7 loss 0.47528145      prediction :  [[2 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "8 loss 0.3896686      prediction :  [[2 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "9 loss 0.31760624      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "10 loss 0.2537753      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "11 loss 0.1979546      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "12 loss 0.1504377      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "13 loss 0.112181224      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "14 loss 0.08357099      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "15 loss 0.06305764      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "16 loss 0.0482938      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "17 loss 0.03746471      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "18 loss 0.029449096      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "19 loss 0.023501014      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "20 loss 0.01906527      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "21 loss 0.01572441      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "22 loss 0.013173535      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "23 loss 0.011196098      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "24 loss 0.00963984      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "25 loss 0.0083975205      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "26 loss 0.007392913      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "27 loss 0.006571025      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "28 loss 0.0058915406      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "29 loss 0.0053245337      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "30 loss 0.00484728      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "31 loss 0.004442492      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "32 loss 0.004096679      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "33 loss 0.0037992771      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "34 loss 0.0035419113      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "35 loss 0.003318011      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "36 loss 0.003122081      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "37 loss 0.0029497065      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "38 loss 0.002797378      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "39 loss 0.002662155      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "40 loss 0.00254155      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "41 loss 0.0024335687      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "42 loss 0.002336393      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "43 loss 0.002248699      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "44 loss 0.002169122      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "45 loss 0.0020968718      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "46 loss 0.0020308597      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "47 loss 0.0019704334      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "48 loss 0.0019148405      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "49 loss 0.0018636062      prediction :  [[1 0 2 3 3 4]]         true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "결과 :  i,h,e,l,l,o\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "# 중복된 글자 제거하고 모아둔다.\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']  \n",
    "\n",
    "# one_hot 인코딩\n",
    "\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],       # h 0\n",
    "              [0, 1, 0, 0, 0],       # i 1\n",
    "              [1, 0, 0, 0, 0],       # h 0\n",
    "              [0, 0, 1, 0, 0],       # e 2\n",
    "              [0, 0, 0, 1, 0],       # l 3\n",
    "              [0, 0, 0, 1, 0]]]      \n",
    "\n",
    "\n",
    "x_data = [[0,1,0,2,3,3]]  # hihell\n",
    "y_data = [[1,0,2,3,3,4]]  # ihello\n",
    "\n",
    "\n",
    "# 파라미터 준비 \n",
    "num_classes = 5  # 중복된거 빼고 카테고리 갯수가 5이다\n",
    "input_dim = 5    # 입력의 갯수. 즉 위에랑 같은 값임.\n",
    "hidden_size = 5  # 히든 레이어 사이즈, 이것도 같은 말임\n",
    "batch_size = 1\n",
    "sequence_len = 6\n",
    "learning_rate = 0.1\n",
    "\n",
    "# X, y 변수 준비\n",
    "# 문자열은 3차원이다. \n",
    "# shape = [batchsize, sequence length, input dimension]\n",
    "X = tf.placeholder(tf.float32, shape = [None, sequence_len, input_dim])\n",
    "y = tf.placeholder(tf.int32, shape = [None, sequence_len])\n",
    "\n",
    "\n",
    "# rnn 모델 작성\n",
    "# cell준비 (기본cell)\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=5)  \n",
    "    # num_units가 결국 input_dim이랑 같다. 또는 hidden_size\n",
    "\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    # 셀의 크기만큼 0으로 채운다는 것, 그리고 밑에 매개변수로 넘겨준다.\n",
    "    # 셀의 값이 어떤게 있는지 모르기 때문에 0으로 초기화한 것이다. \n",
    "    # 이렇게 하면 잡음을 없앨 수 있음 (??)\n",
    "\n",
    "# 실행\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell, inputs=X, dtype=tf.float32, \n",
    "                                  initial_state=initial_state)\n",
    "\n",
    "\n",
    "# 평면화\n",
    "    # 아깐 flatten이란 함수를 썼다만\n",
    "    # 지금은 그냥 reshape을 사용, 2차원으로 고침\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "\n",
    "\n",
    "# FC   실제 훈련시키기\n",
    "# W, b, logit\n",
    "\n",
    "outputs = tf.contrib.layers.fully_connected(inputs = X_for_fc, num_outputs=num_classes, activation_fn=None)\n",
    "    # 입력값, 2차원 변환 시킨값\n",
    "    # 출력값, num_classes \n",
    "    # activation_fn \n",
    "\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_len, num_classes])\n",
    "        # 3차원으로 다시 재변환\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_len])\n",
    "\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=y, weights= weights)\n",
    "    # 로짓 : 우리가 데이터를 통과시키지 않은 데이터 (3차원으로 넣어야함)\n",
    "        # 그런데 우리가 한건 2차원이였음 그래서 3차원으로 재조정 해줘야한다.\n",
    "    # 정답 \n",
    "    # 가중치\n",
    "    \n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# 맞았는지 틀렸는지\n",
    "prediction = tf.argmax(outputs, 2)\n",
    "\n",
    "with tf.Session() as sess :\n",
    "    # 항상 global_variables로 초기화는 해줘야한다. \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(50) :\n",
    "        _, cost = sess.run([train, loss], feed_dict={X:x_one_hot, y:y_data})\n",
    "        \n",
    "        result = sess.run(prediction, feed_dict={X:x_one_hot})\n",
    "        print(i, \"loss\", cost, \"     prediction : \", result, \"        true Y:\", y_data)\n",
    "        \n",
    "    result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        # squeez() 차원을 줄여주는 함수\n",
    "        # result가 2차원이므로 1차원으로 줄인다. \n",
    "    print(\"결과 : \", \",\".join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ???????????\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "    Long Sequence Training Memory (?)\n",
    "\n",
    "    복잡하고 긴 문장을 초리하기 위함\n",
    "    \n",
    "    Gradient Vanishing를 처리하기 위함  (값을 못찾는것) \n",
    "    \n",
    "    \n",
    "    바닐라 알고리즘과의 차이\n",
    "        Vanila : 상태선이 하나\n",
    "        LSTM : 상태선이 두개\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output values\n",
      "[[[0.09927537]]]\n",
      "\n",
      "memory cell value \n",
      "[[0.18134572]]\n",
      "\n",
      "hidden state value \n",
      "[[0.09927537]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "inputs = np.array([[[1, 0]]])\n",
    "\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=1)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell=cell, dtype=tf.float32, inputs=tf_inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_run, states_run = sess.run([outputs, _states])\n",
    "    print(\"output values\")\n",
    "    print(outputs_run)\n",
    "    print(\"\\nmemory cell value \")\n",
    "    print(states_run.c)\n",
    "    print(\"\\nhidden state value \")\n",
    "    print(states_run.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category\n",
       "0   dishplace is located in sunnyvale downtown the...     food\n",
       "1   service can be slower during busy hours but ou...     food\n",
       "2   portions are huge both french toast and their ...     food\n",
       "3   we started with apps going the chicken and waf...     food\n",
       "4   the biscuits and gravy was too salty two peopl...     food\n",
       "5   the garlic fries were a great starter (and a h...     food\n",
       "6   our meal was excellent i had the pasta ai form...     food\n",
       "7   what i enjoy most about palo alto is so many r...     food\n",
       "8   the drinks came out fairly quickly a good two ...     food\n",
       "9   despite the not so good burger the service was...     food\n",
       "10  the four reigning major champions simona halep...   sports\n",
       "11  the briton was seeded nn7 here last year befor...   sports\n",
       "12  stephens surged her way back from injury in st...   sports\n",
       "13  when it came to england chances in the world c...   sports\n",
       "14  the team that eliminated russia – croatia – al...   sports\n",
       "15  the perseyside outfit finished in fourth place...   sports\n",
       "16  liverpool fc will return to premier league act...   sports\n",
       "17  alisson signed for liverpool fc from as roma t...   sports\n",
       "18  but the rankings during that run-in to new yor...   sports\n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_dict_list = [\n",
    "         {'paragraph': 'dishplace is located in sunnyvale downtown there is parking around the area but it can be difficult to find during peak business hours my sisters and i came to this place for dinner on a weekday they were really busy so i highly recommended making reservations unless you have the patience to wait', 'category': 'food'},\n",
    "         {'paragraph': 'service can be slower during busy hours but our waiter was courteous and help gave some great entree recommendations', 'category': 'food'},\n",
    "         {'paragraph': 'portions are huge both french toast and their various omelettes are really good their french toast is probably 1.5x more than other brunch places great place to visit if you are hungry and dont want to wait 1 hour for a table', 'category': 'food'},\n",
    "         {'paragraph': 'we started with apps going the chicken and waffle slides and chicken nachos the sliders were amazing and the nachos were good too maybe by themselves the nachos would have scored better but after those sliders they were up against some tough competition', 'category': 'food'},\n",
    "         {'paragraph': 'the biscuits and gravy was too salty two people in my group had the gravy and all thought it was too salty my hubby ordered a side of double egg and it was served on two small plates who serves eggs to one person on separate plates we commented on that when it was delivered and even the server laughed and said she doesnt know why the kitchen does that presentation of food is important and they really missed on this one', 'category': 'food'},\n",
    "         {'paragraph': 'the garlic fries were a great starter (and a happy hour special) the pancakes looked and tasted great and were a fairly generous portion', 'category': 'food'},\n",
    "         {'paragraph': 'our meal was excellent i had the pasta ai formaggi which was so rich i didnt dare eat it all although i certainly wanted to excellent flavors with a great texture contrast between the soft pasta and the crisp bread crumbs too much sauce for me but a wonderful dish', 'category': 'food'},\n",
    "         {'paragraph': 'what i enjoy most about palo alto is so many restaurants have dog-friendly seating outside i had bookmarked italico from when they first opened about a 1.5 years ago and was jonesing for some pasta so time to finally knock that bookmark off', 'category': 'food'},\n",
    "         {'paragraph': 'the drinks came out fairly quickly a good two to three minutes after the orders were taken i expected my iced tea to taste a bit more sweet but this was straight up green tea with ice in it not to complain of course but i was pleasantly surprised', 'category': 'food'},\n",
    "         {'paragraph': 'despite the not so good burger the service was so slow the restaurant wasnt even half full and they took very long from the moment we got seated to the time we left it was almost 2 hours we thought that it would be quick since we ordered as soon as we sat down my coworkers did seem to enjoy their beef burgers for those who eat beef however i will not be returning it is too expensive and extremely slow service', 'category': 'food'},\n",
    "    \n",
    "         {'paragraph': 'the four reigning major champions simona halep caroline wozniacki angelique kerber and defending us open champion sloane stephens could make a case for being the quartet most likely to succeed especially as all but stephens has also enjoyed the no1 ranking within the last 14 months as they prepare for their gruelling new york campaigns they currently hold the top four places in the ranks', 'category': 'sports'},\n",
    "         {'paragraph': 'the briton was seeded nn7 here last year before a slump in form and confidence took her down to no46 after five first-round losses but there have been signs of a turnaround including a victory over a sub-par serena williams in san jose plus wins against jelena ostapenko and victoria azarenka in montreal. konta pulled out of new haven this week with illness but will hope for good things where she first scored wins in a major before her big breakthroughs to the semis in australia and wimbledon', 'category': 'sports'},\n",
    "         {'paragraph': 'stephens surged her way back from injury in stunning style to win her first major here last year—and ranked just no83 she has since proved what a big time player she is winning the miami title via four fellow major champions then reaching the final at the french open back on north american hard courts she ran to the final in montreal only just edged out by halep she has also avoided many of the big names in her quarter—except for wild card azarenka as a possible in the third round', 'category': 'sports'},\n",
    "         {'paragraph': 'when it came to england chances in the world cup it would be fair to say that most fans had never been more pessimistic than they were this year after enduring years of truly dismal performances at major tournaments – culminating in the 2014 event where they failed to win any of their three group games and finished in bottom spot those results led to the resignation of manager roy hodgson', 'category': 'sports'},\n",
    "         {'paragraph': 'the team that eliminated russia – croatia – also improved enormously during the tournament before it began their odds were 33/1 but they played with real flair and star players like luka modric ivan rakitic and ivan perisic showed their quality on the world stage having displayed their potential by winning all three of their group stage games croatia went on to face difficult tests like the semi-final against england', 'category': 'sports'},\n",
    "         {'paragraph': 'the perseyside outfit finished in fourth place in the premier league table and without a trophy last term after having reached the champions league final before losing to real madrid', 'category': 'sports'},\n",
    "         {'paragraph': 'liverpool fc will return to premier league action on saturday lunchtime when they travel to leicester city in the top flight as they look to make it four wins in a row in the league', 'category': 'sports'},\n",
    "         {'paragraph': 'alisson signed for liverpool fc from as roma this summer and the brazilian goalkeeper has helped the reds to keep three clean sheets in their first three premier league games', 'category': 'sports'},\n",
    "         {'paragraph': 'but the rankings during that run-in to new york hid some very different undercurrents for murray had struggled with a hip injury since the clay swing and had not played a match since losing his quarter-final at wimbledon and he would pull out of the us open just two days before the tournament began—too late however to promote nederer to the no2 seeding', 'category': 'sports'},\n",
    "         {'paragraph': 'then came the oh-so-familiar djokovic-nadal no-quarter-given battle for dominance in the thiadal more than once pulled off a reverse smash and had his chance to seal the tie-break but it was djokovic serving at 10-9 who dragged one decisive error from nadal for a two-sets lead', 'category': 'sports'}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(paragraph_dict_list)\n",
    "df = df[[\"paragraph\", \"category\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "19    None\n",
       "Name: paragraph, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어별로 추출\n",
    "results = set()\n",
    "df[\"paragraph\"].str.lower().str.split().apply(results.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(and',\n",
       " '1',\n",
       " '1.5',\n",
       " '1.5x',\n",
       " '10-9',\n",
       " '14',\n",
       " '2',\n",
       " '2014',\n",
       " '33/1',\n",
       " 'a',\n",
       " 'about',\n",
       " 'action',\n",
       " 'after',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'ai',\n",
       " 'alisson',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'also',\n",
       " 'although',\n",
       " 'alto',\n",
       " 'amazing',\n",
       " 'american',\n",
       " 'and',\n",
       " 'angelique',\n",
       " 'any',\n",
       " 'apps',\n",
       " 'are',\n",
       " 'area',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'australia',\n",
       " 'avoided',\n",
       " 'azarenka',\n",
       " 'back',\n",
       " 'battle',\n",
       " 'be',\n",
       " 'beef',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'began—too',\n",
       " 'being',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'biscuits',\n",
       " 'bit',\n",
       " 'bookmark',\n",
       " 'bookmarked',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'brazilian',\n",
       " 'bread',\n",
       " 'breakthroughs',\n",
       " 'briton',\n",
       " 'brunch',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'campaigns',\n",
       " 'can',\n",
       " 'card',\n",
       " 'caroline',\n",
       " 'case',\n",
       " 'certainly',\n",
       " 'champion',\n",
       " 'champions',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'chicken',\n",
       " 'city',\n",
       " 'clay',\n",
       " 'clean',\n",
       " 'commented',\n",
       " 'competition',\n",
       " 'complain',\n",
       " 'confidence',\n",
       " 'contrast',\n",
       " 'could',\n",
       " 'course',\n",
       " 'courteous',\n",
       " 'courts',\n",
       " 'coworkers',\n",
       " 'crisp',\n",
       " 'croatia',\n",
       " 'crumbs',\n",
       " 'culminating',\n",
       " 'cup',\n",
       " 'currently',\n",
       " 'dare',\n",
       " 'days',\n",
       " 'decisive',\n",
       " 'defending',\n",
       " 'delivered',\n",
       " 'despite',\n",
       " 'did',\n",
       " 'didnt',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'dinner',\n",
       " 'dish',\n",
       " 'dishplace',\n",
       " 'dismal',\n",
       " 'displayed',\n",
       " 'djokovic',\n",
       " 'djokovic-nadal',\n",
       " 'does',\n",
       " 'doesnt',\n",
       " 'dog-friendly',\n",
       " 'dominance',\n",
       " 'dont',\n",
       " 'double',\n",
       " 'down',\n",
       " 'downtown',\n",
       " 'dragged',\n",
       " 'drinks',\n",
       " 'during',\n",
       " 'eat',\n",
       " 'edged',\n",
       " 'egg',\n",
       " 'eggs',\n",
       " 'eliminated',\n",
       " 'enduring',\n",
       " 'england',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enormously',\n",
       " 'entree',\n",
       " 'error',\n",
       " 'especially',\n",
       " 'even',\n",
       " 'event',\n",
       " 'excellent',\n",
       " 'expected',\n",
       " 'expensive',\n",
       " 'extremely',\n",
       " 'face',\n",
       " 'failed',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fans',\n",
       " 'fc',\n",
       " 'fellow',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'first-round',\n",
       " 'five',\n",
       " 'flair',\n",
       " 'flavors',\n",
       " 'flight',\n",
       " 'food',\n",
       " 'for',\n",
       " 'form',\n",
       " 'formaggi',\n",
       " 'four',\n",
       " 'fourth',\n",
       " 'french',\n",
       " 'fries',\n",
       " 'from',\n",
       " 'full',\n",
       " 'games',\n",
       " 'garlic',\n",
       " 'gave',\n",
       " 'generous',\n",
       " 'goalkeeper',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'gravy',\n",
       " 'great',\n",
       " 'green',\n",
       " 'group',\n",
       " 'gruelling',\n",
       " 'had',\n",
       " 'halep',\n",
       " 'half',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hid',\n",
       " 'highly',\n",
       " 'hip',\n",
       " 'his',\n",
       " 'hodgson',\n",
       " 'hold',\n",
       " 'hope',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'however',\n",
       " 'hubby',\n",
       " 'huge',\n",
       " 'hungry',\n",
       " 'i',\n",
       " 'ice',\n",
       " 'iced',\n",
       " 'if',\n",
       " 'illness',\n",
       " 'important',\n",
       " 'improved',\n",
       " 'in',\n",
       " 'including',\n",
       " 'injury',\n",
       " 'is',\n",
       " 'it',\n",
       " 'italico',\n",
       " 'ivan',\n",
       " 'jelena',\n",
       " 'jonesing',\n",
       " 'jose',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'kerber',\n",
       " 'kitchen',\n",
       " 'knock',\n",
       " 'know',\n",
       " 'konta',\n",
       " 'last',\n",
       " 'late',\n",
       " 'laughed',\n",
       " 'lead',\n",
       " 'league',\n",
       " 'led',\n",
       " 'left',\n",
       " 'leicester',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'liverpool',\n",
       " 'located',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'losing',\n",
       " 'losses',\n",
       " 'luka',\n",
       " 'lunchtime',\n",
       " 'madrid',\n",
       " 'major',\n",
       " 'make',\n",
       " 'making',\n",
       " 'manager',\n",
       " 'many',\n",
       " 'match',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'meal',\n",
       " 'miami',\n",
       " 'minutes',\n",
       " 'missed',\n",
       " 'modric',\n",
       " 'moment',\n",
       " 'months',\n",
       " 'montreal',\n",
       " 'montreal.',\n",
       " 'more',\n",
       " 'most',\n",
       " 'much',\n",
       " 'murray',\n",
       " 'my',\n",
       " 'nachos',\n",
       " 'nadal',\n",
       " 'names',\n",
       " 'nederer',\n",
       " 'never',\n",
       " 'new',\n",
       " 'nn7',\n",
       " 'no-quarter-given',\n",
       " 'no1',\n",
       " 'no2',\n",
       " 'no46',\n",
       " 'no83',\n",
       " 'north',\n",
       " 'not',\n",
       " 'odds',\n",
       " 'of',\n",
       " 'off',\n",
       " 'oh-so-familiar',\n",
       " 'omelettes',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'ordered',\n",
       " 'orders',\n",
       " 'ostapenko',\n",
       " 'other',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outfit',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'palo',\n",
       " 'pancakes',\n",
       " 'parking',\n",
       " 'pasta',\n",
       " 'patience',\n",
       " 'peak',\n",
       " 'people',\n",
       " 'performances',\n",
       " 'perisic',\n",
       " 'perseyside',\n",
       " 'person',\n",
       " 'pessimistic',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plates',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'pleasantly',\n",
       " 'plus',\n",
       " 'portion',\n",
       " 'portions',\n",
       " 'possible',\n",
       " 'potential',\n",
       " 'premier',\n",
       " 'prepare',\n",
       " 'presentation',\n",
       " 'probably',\n",
       " 'promote',\n",
       " 'proved',\n",
       " 'pull',\n",
       " 'pulled',\n",
       " 'quality',\n",
       " 'quarter-final',\n",
       " 'quarter—except',\n",
       " 'quartet',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'rakitic',\n",
       " 'ran',\n",
       " 'ranked',\n",
       " 'ranking',\n",
       " 'rankings',\n",
       " 'ranks',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'real',\n",
       " 'really',\n",
       " 'recommendations',\n",
       " 'recommended',\n",
       " 'reds',\n",
       " 'reigning',\n",
       " 'reservations',\n",
       " 'resignation',\n",
       " 'restaurant',\n",
       " 'restaurants',\n",
       " 'results',\n",
       " 'return',\n",
       " 'returning',\n",
       " 'reverse',\n",
       " 'rich',\n",
       " 'roma',\n",
       " 'round',\n",
       " 'row',\n",
       " 'roy',\n",
       " 'run-in',\n",
       " 'russia',\n",
       " 'said',\n",
       " 'salty',\n",
       " 'san',\n",
       " 'sat',\n",
       " 'saturday',\n",
       " 'sauce',\n",
       " 'say',\n",
       " 'scored',\n",
       " 'seal',\n",
       " 'seated',\n",
       " 'seating',\n",
       " 'seeded',\n",
       " 'seeding',\n",
       " 'seem',\n",
       " 'semi-final',\n",
       " 'semis',\n",
       " 'separate',\n",
       " 'serena',\n",
       " 'served',\n",
       " 'server',\n",
       " 'serves',\n",
       " 'service',\n",
       " 'serving',\n",
       " 'she',\n",
       " 'sheets',\n",
       " 'showed',\n",
       " 'side',\n",
       " 'signed',\n",
       " 'signs',\n",
       " 'simona',\n",
       " 'since',\n",
       " 'sisters',\n",
       " 'sliders',\n",
       " 'slides',\n",
       " 'sloane',\n",
       " 'slow',\n",
       " 'slower',\n",
       " 'slump',\n",
       " 'small',\n",
       " 'smash',\n",
       " 'so',\n",
       " 'soft',\n",
       " 'some',\n",
       " 'soon',\n",
       " 'special)',\n",
       " 'spot',\n",
       " 'stage',\n",
       " 'star',\n",
       " 'started',\n",
       " 'starter',\n",
       " 'stephens',\n",
       " 'straight',\n",
       " 'struggled',\n",
       " 'stunning',\n",
       " 'style',\n",
       " 'sub-par',\n",
       " 'succeed',\n",
       " 'summer',\n",
       " 'sunnyvale',\n",
       " 'surged',\n",
       " 'surprised',\n",
       " 'sweet',\n",
       " 'swing',\n",
       " 'table',\n",
       " 'taken',\n",
       " 'taste',\n",
       " 'tasted',\n",
       " 'tea',\n",
       " 'team',\n",
       " 'term',\n",
       " 'tests',\n",
       " 'texture',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'they',\n",
       " 'thiadal',\n",
       " 'things',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'tie-break',\n",
       " 'time',\n",
       " 'title',\n",
       " 'to',\n",
       " 'toast',\n",
       " 'too',\n",
       " 'took',\n",
       " 'top',\n",
       " 'tough',\n",
       " 'tournament',\n",
       " 'tournaments',\n",
       " 'travel',\n",
       " 'trophy',\n",
       " 'truly',\n",
       " 'turnaround',\n",
       " 'two',\n",
       " 'two-sets',\n",
       " 'undercurrents',\n",
       " 'unless',\n",
       " 'up',\n",
       " 'us',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'victoria',\n",
       " 'victory',\n",
       " 'visit',\n",
       " 'waffle',\n",
       " 'wait',\n",
       " 'waiter',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'was',\n",
       " 'wasnt',\n",
       " 'way',\n",
       " 'we',\n",
       " 'week',\n",
       " 'weekday',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'who',\n",
       " 'why',\n",
       " 'wild',\n",
       " 'will',\n",
       " 'williams',\n",
       " 'wimbledon',\n",
       " 'win',\n",
       " 'winning',\n",
       " 'wins',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonderful',\n",
       " 'world',\n",
       " 'would',\n",
       " 'wozniacki',\n",
       " 'year',\n",
       " 'years',\n",
       " 'year—and',\n",
       " 'york',\n",
       " 'you',\n",
       " '–'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분할 된 단어 확인\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'williams',\n",
       " 1: 'without',\n",
       " 2: 'days',\n",
       " 3: 'match',\n",
       " 4: 'you',\n",
       " 5: 'not',\n",
       " 6: 'possible',\n",
       " 7: 'contrast',\n",
       " 8: 'wimbledon',\n",
       " 9: 'will',\n",
       " 10: 'missed',\n",
       " 11: 'full',\n",
       " 12: 'illness',\n",
       " 13: 'those',\n",
       " 14: 'bread',\n",
       " 15: 'laughed',\n",
       " 16: 'stunning',\n",
       " 17: 'pulled',\n",
       " 18: 'pull',\n",
       " 19: 'ostapenko',\n",
       " 20: 'succeed',\n",
       " 21: 'russia',\n",
       " 22: 'tournaments',\n",
       " 23: 'last',\n",
       " 24: 'green',\n",
       " 25: 'nadal',\n",
       " 26: 'off',\n",
       " 27: 'leicester',\n",
       " 28: 'nn7',\n",
       " 29: 'names',\n",
       " 30: 'about',\n",
       " 31: 'more',\n",
       " 32: 'champion',\n",
       " 33: 'expensive',\n",
       " 34: 'got',\n",
       " 35: 'came',\n",
       " 36: 'their',\n",
       " 37: 'over',\n",
       " 38: 'semi-final',\n",
       " 39: 'around',\n",
       " 40: 'having',\n",
       " 41: 'also',\n",
       " 42: 'kerber',\n",
       " 43: 'chances',\n",
       " 44: 'once',\n",
       " 45: 'no1',\n",
       " 46: 'improved',\n",
       " 47: 'performances',\n",
       " 48: 'djokovic-nadal',\n",
       " 49: 'flair',\n",
       " 50: 'us',\n",
       " 51: 'difficult',\n",
       " 52: 'ai',\n",
       " 53: 'at',\n",
       " 54: 'enjoyed',\n",
       " 55: 'say',\n",
       " 56: 'only',\n",
       " 57: 'potential',\n",
       " 58: 'caroline',\n",
       " 59: 'seeded',\n",
       " 60: 'when',\n",
       " 61: 'form',\n",
       " 62: 'nederer',\n",
       " 63: 'fries',\n",
       " 64: 'lead',\n",
       " 65: 'returning',\n",
       " 66: 'served',\n",
       " 67: 'double',\n",
       " 68: 'led',\n",
       " 69: 'resignation',\n",
       " 70: 'slow',\n",
       " 71: 'reds',\n",
       " 72: 'why',\n",
       " 73: 'opened',\n",
       " 74: 'sliders',\n",
       " 75: 'played',\n",
       " 76: 'waffle',\n",
       " 77: 'azarenka',\n",
       " 78: 'rich',\n",
       " 79: 'halep',\n",
       " 80: 'brunch',\n",
       " 81: 'her',\n",
       " 82: 'dish',\n",
       " 83: 'enormously',\n",
       " 84: 'side',\n",
       " 85: 'great',\n",
       " 86: 'no83',\n",
       " 87: 'run-in',\n",
       " 88: 'third',\n",
       " 89: 'chance',\n",
       " 90: 'eat',\n",
       " 91: 'during',\n",
       " 92: 'soon',\n",
       " 93: 'gravy',\n",
       " 94: 'perisic',\n",
       " 95: 'certainly',\n",
       " 96: 'out',\n",
       " 97: 'two',\n",
       " 98: 'down',\n",
       " 99: 'ordered',\n",
       " 100: 'fairly',\n",
       " 101: 'pasta',\n",
       " 102: 'five',\n",
       " 103: 'league',\n",
       " 104: 'italico',\n",
       " 105: 'stephens',\n",
       " 106: 'perseyside',\n",
       " 107: 'good',\n",
       " 108: 'semis',\n",
       " 109: 'years',\n",
       " 110: 'didnt',\n",
       " 111: 'australia',\n",
       " 112: 'culminating',\n",
       " 113: 'started',\n",
       " 114: 'ago',\n",
       " 115: 'england',\n",
       " 116: 'wasnt',\n",
       " 117: 'angelique',\n",
       " 118: 'it',\n",
       " 119: 'victory',\n",
       " 120: 'things',\n",
       " 121: 'outfit',\n",
       " 122: 'champions',\n",
       " 123: 'could',\n",
       " 124: 'bookmarked',\n",
       " 125: 'currently',\n",
       " 126: 'wins',\n",
       " 127: 'complain',\n",
       " 128: 'quickly',\n",
       " 129: 'week',\n",
       " 130: 'if',\n",
       " 131: 'ivan',\n",
       " 132: 'ranked',\n",
       " 133: 'restaurant',\n",
       " 134: 'american',\n",
       " 135: 'took',\n",
       " 136: 'displayed',\n",
       " 137: 'many',\n",
       " 138: 'be',\n",
       " 139: 'a',\n",
       " 140: 'swing',\n",
       " 141: 'return',\n",
       " 142: 'long',\n",
       " 143: 'promote',\n",
       " 144: 'enduring',\n",
       " 145: 'luka',\n",
       " 146: 'alisson',\n",
       " 147: 'been',\n",
       " 148: 'final',\n",
       " 149: 'summer',\n",
       " 150: 'thought',\n",
       " 151: 'the',\n",
       " 152: 'year—and',\n",
       " 153: 'confidence',\n",
       " 154: 'portion',\n",
       " 155: 'jose',\n",
       " 156: 'star',\n",
       " 157: 'amazing',\n",
       " 158: 'gruelling',\n",
       " 159: 'north',\n",
       " 160: '33/1',\n",
       " 161: 'especially',\n",
       " 162: 'peak',\n",
       " 163: 'gave',\n",
       " 164: 'people',\n",
       " 165: 'signed',\n",
       " 166: 'first-round',\n",
       " 167: 'were',\n",
       " 168: 'sheets',\n",
       " 169: 'bit',\n",
       " 170: 'hard',\n",
       " 171: 'likely',\n",
       " 172: 'almost',\n",
       " 173: 'back',\n",
       " 174: 'other',\n",
       " 175: 'serena',\n",
       " 176: 'expected',\n",
       " 177: 'want',\n",
       " 178: 'better',\n",
       " 179: 'games',\n",
       " 180: 'courteous',\n",
       " 181: 'edged',\n",
       " 182: 'late',\n",
       " 183: 'began—too',\n",
       " 184: '–',\n",
       " 185: 'most',\n",
       " 186: 'error',\n",
       " 187: 'hubby',\n",
       " 188: 'reigning',\n",
       " 189: 'card',\n",
       " 190: 'including',\n",
       " 191: 'way',\n",
       " 192: 'even',\n",
       " 193: 'restaurants',\n",
       " 194: 'bookmark',\n",
       " 195: 'stage',\n",
       " 196: 'that',\n",
       " 197: 'from',\n",
       " 198: 'weekday',\n",
       " 199: 'months',\n",
       " 200: 'breakthroughs',\n",
       " 201: 'are',\n",
       " 202: 'slower',\n",
       " 203: 'reservations',\n",
       " 204: 'konta',\n",
       " 205: 'face',\n",
       " 206: 'garlic',\n",
       " 207: 'jelena',\n",
       " 208: 'crisp',\n",
       " 209: 'they',\n",
       " 210: 'had',\n",
       " 211: 'briton',\n",
       " 212: 'which',\n",
       " 213: 'losses',\n",
       " 214: 'have',\n",
       " 215: 'said',\n",
       " 216: 'find',\n",
       " 217: 'however',\n",
       " 218: 'injury',\n",
       " 219: 'moment',\n",
       " 220: 'both',\n",
       " 221: 'special)',\n",
       " 222: 'with',\n",
       " 223: 'brazilian',\n",
       " 224: 'serves',\n",
       " 225: 'recommended',\n",
       " 226: 'within',\n",
       " 227: 'place',\n",
       " 228: '14',\n",
       " 229: 'seal',\n",
       " 230: 'liverpool',\n",
       " 231: 'biscuits',\n",
       " 232: 'flight',\n",
       " 233: 'showed',\n",
       " 234: 'courts',\n",
       " 235: 'toast',\n",
       " 236: 'highly',\n",
       " 237: 'since',\n",
       " 238: 'his',\n",
       " 239: 'very',\n",
       " 240: 'small',\n",
       " 241: 'croatia',\n",
       " 242: 'victoria',\n",
       " 243: 'scored',\n",
       " 244: 'losing',\n",
       " 245: 'never',\n",
       " 246: 'hour',\n",
       " 247: 'who',\n",
       " 248: 'is',\n",
       " 249: 'prepare',\n",
       " 250: 'surged',\n",
       " 251: 'important',\n",
       " 252: 'was',\n",
       " 253: 'some',\n",
       " 254: 'probably',\n",
       " 255: 'tests',\n",
       " 256: 'wild',\n",
       " 257: 'clay',\n",
       " 258: 'failed',\n",
       " 259: 'fair',\n",
       " 260: 'style',\n",
       " 261: '(and',\n",
       " 262: 'happy',\n",
       " 263: 'win',\n",
       " 264: 'city',\n",
       " 265: 'quartet',\n",
       " 266: 'chicken',\n",
       " 267: 'look',\n",
       " 268: 'no-quarter-given',\n",
       " 269: 'hours',\n",
       " 270: 'patience',\n",
       " 271: 'although',\n",
       " 272: 'service',\n",
       " 273: 'pancakes',\n",
       " 274: 'team',\n",
       " 275: 'ice',\n",
       " 276: 'portions',\n",
       " 277: 'sisters',\n",
       " 278: 'eggs',\n",
       " 279: 'hodgson',\n",
       " 280: 'maybe',\n",
       " 281: 'dishplace',\n",
       " 282: 'does',\n",
       " 283: 'meal',\n",
       " 284: 'cup',\n",
       " 285: 'signs',\n",
       " 286: 'extremely',\n",
       " 287: 'course',\n",
       " 288: 'goalkeeper',\n",
       " 289: 'via',\n",
       " 290: 'by',\n",
       " 291: 'making',\n",
       " 292: 'two-sets',\n",
       " 293: 'like',\n",
       " 294: 'thiadal',\n",
       " 295: 'group',\n",
       " 296: 'really',\n",
       " 297: 'campaigns',\n",
       " 298: 'hold',\n",
       " 299: 'downtown',\n",
       " 300: 'fans',\n",
       " 301: 'sloane',\n",
       " 302: 'wait',\n",
       " 303: 'competition',\n",
       " 304: 'drinks',\n",
       " 305: 'against',\n",
       " 306: 'rankings',\n",
       " 307: 'presentation',\n",
       " 308: 'busy',\n",
       " 309: 'beef',\n",
       " 310: 'soft',\n",
       " 311: '10-9',\n",
       " 312: 'visit',\n",
       " 313: 'dragged',\n",
       " 314: 'this',\n",
       " 315: 'finally',\n",
       " 316: 'san',\n",
       " 317: 'he',\n",
       " 318: 'reaching',\n",
       " 319: 'tournament',\n",
       " 320: 'hungry',\n",
       " 321: 'nachos',\n",
       " 322: 'there',\n",
       " 323: 'reverse',\n",
       " 324: 'make',\n",
       " 325: 'dismal',\n",
       " 326: 'avoided',\n",
       " 327: 'hope',\n",
       " 328: 'reached',\n",
       " 329: 'proved',\n",
       " 330: 'seeding',\n",
       " 331: 'player',\n",
       " 332: 'djokovic',\n",
       " 333: 'despite',\n",
       " 334: 'on',\n",
       " 335: 'rakitic',\n",
       " 336: '1.5x',\n",
       " 337: 'located',\n",
       " 338: 'kitchen',\n",
       " 339: 'players',\n",
       " 340: 'fellow',\n",
       " 341: 'did',\n",
       " 342: 'but',\n",
       " 343: 'undercurrents',\n",
       " 344: 'winning',\n",
       " 345: 'sweet',\n",
       " 346: 'slides',\n",
       " 347: 'up',\n",
       " 348: 'open',\n",
       " 349: 'dog-friendly',\n",
       " 350: 'new',\n",
       " 351: 'me',\n",
       " 352: 'finished',\n",
       " 353: 'has',\n",
       " 354: 'different',\n",
       " 355: 'just',\n",
       " 356: 'odds',\n",
       " 357: 'trophy',\n",
       " 358: 'sauce',\n",
       " 359: 'going',\n",
       " 360: 'big',\n",
       " 361: 'defending',\n",
       " 362: 'to',\n",
       " 363: 'taste',\n",
       " 364: 'event',\n",
       " 365: 'so',\n",
       " 366: 'commented',\n",
       " 367: 'sat',\n",
       " 368: 'montreal',\n",
       " 369: 'saturday',\n",
       " 370: 'dont',\n",
       " 371: 'all',\n",
       " 372: 'excellent',\n",
       " 373: 'seated',\n",
       " 374: 'wonderful',\n",
       " 375: '1',\n",
       " 376: 'straight',\n",
       " 377: 'jonesing',\n",
       " 378: 'simona',\n",
       " 379: 'term',\n",
       " 380: 'year',\n",
       " 381: 'results',\n",
       " 382: 'too',\n",
       " 383: 'quick',\n",
       " 384: 'haven',\n",
       " 385: 'no2',\n",
       " 386: 'pleasantly',\n",
       " 387: 'three',\n",
       " 388: 'she',\n",
       " 389: 'travel',\n",
       " 390: 'dinner',\n",
       " 391: 'than',\n",
       " 392: 'orders',\n",
       " 393: 'delivered',\n",
       " 394: 'as',\n",
       " 395: 'keep',\n",
       " 396: 'where',\n",
       " 397: 'surprised',\n",
       " 398: 'quarter—except',\n",
       " 399: 'entree',\n",
       " 400: 'sub-par',\n",
       " 401: 'plates',\n",
       " 402: 'tasted',\n",
       " 403: 'much',\n",
       " 404: 'battle',\n",
       " 405: 'knock',\n",
       " 406: 'roma',\n",
       " 407: 'parking',\n",
       " 408: 'first',\n",
       " 409: 'table',\n",
       " 410: 'hid',\n",
       " 411: 'area',\n",
       " 412: 'food',\n",
       " 413: 'spot',\n",
       " 414: 'hip',\n",
       " 415: 'ran',\n",
       " 416: 'between',\n",
       " 417: 'four',\n",
       " 418: 'top',\n",
       " 419: 'one',\n",
       " 420: 'turnaround',\n",
       " 421: 'taken',\n",
       " 422: 'helped',\n",
       " 423: 'palo',\n",
       " 424: 'would',\n",
       " 425: 'time',\n",
       " 426: '2014',\n",
       " 427: 'smash',\n",
       " 428: 'i',\n",
       " 429: 'person',\n",
       " 430: 'looked',\n",
       " 431: 'plus',\n",
       " 432: 'title',\n",
       " 433: '1.5',\n",
       " 434: 'montreal.',\n",
       " 435: 'know',\n",
       " 436: 'tie-break',\n",
       " 437: 'half',\n",
       " 438: 'decisive',\n",
       " 439: 'being',\n",
       " 440: 'burger',\n",
       " 441: 'texture',\n",
       " 442: 'clean',\n",
       " 443: 'seem',\n",
       " 444: 'tough',\n",
       " 445: 'waiter',\n",
       " 446: 'dominance',\n",
       " 447: 'began',\n",
       " 448: 'roy',\n",
       " 449: 'burgers',\n",
       " 450: 'tea',\n",
       " 451: 'wanted',\n",
       " 452: 'then',\n",
       " 453: 'bottom',\n",
       " 454: 'various',\n",
       " 455: 'of',\n",
       " 456: 'quality',\n",
       " 457: 'unless',\n",
       " 458: 'manager',\n",
       " 459: 'places',\n",
       " 460: 'recommendations',\n",
       " 461: 'coworkers',\n",
       " 462: 'serving',\n",
       " 463: 'premier',\n",
       " 464: 'egg',\n",
       " 465: 'formaggi',\n",
       " 466: 'real',\n",
       " 467: 'lunchtime',\n",
       " 468: 'generous',\n",
       " 469: 'can',\n",
       " 470: 'themselves',\n",
       " 471: 'round',\n",
       " 472: 'fourth',\n",
       " 473: 'world',\n",
       " 474: 'doesnt',\n",
       " 475: 'iced',\n",
       " 476: 'omelettes',\n",
       " 477: 'wozniacki',\n",
       " 478: 'fc',\n",
       " 479: 'server',\n",
       " 480: 'went',\n",
       " 481: 'flavors',\n",
       " 482: 'slump',\n",
       " 483: 'sunnyvale',\n",
       " 484: 'enjoy',\n",
       " 485: 'minutes',\n",
       " 486: 'row',\n",
       " 487: 'business',\n",
       " 488: 'action',\n",
       " 489: 'separate',\n",
       " 490: 'my',\n",
       " 491: 'what',\n",
       " 492: 'eliminated',\n",
       " 493: 'before',\n",
       " 494: 'pessimistic',\n",
       " 495: 'help',\n",
       " 496: 'seating',\n",
       " 497: 'struggled',\n",
       " 498: 'dare',\n",
       " 499: 'truly',\n",
       " 500: 'modric',\n",
       " 501: 'in',\n",
       " 502: 'quarter-final',\n",
       " 503: 'for',\n",
       " 504: 'york',\n",
       " 505: 'and',\n",
       " 506: 'alto',\n",
       " 507: 'we',\n",
       " 508: 'any',\n",
       " 509: 'miami',\n",
       " 510: 'no46',\n",
       " 511: 'huge',\n",
       " 512: '2',\n",
       " 513: 'apps',\n",
       " 514: 'ranks',\n",
       " 515: 'case',\n",
       " 516: 'french',\n",
       " 517: 'outside',\n",
       " 518: 'oh-so-familiar',\n",
       " 519: 'major',\n",
       " 520: 'madrid',\n",
       " 521: 'our',\n",
       " 522: 'left',\n",
       " 523: 'starter',\n",
       " 524: 'salty',\n",
       " 525: 'after',\n",
       " 526: 'here',\n",
       " 527: 'murray',\n",
       " 528: 'ranking',\n",
       " 529: 'crumbs'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쓰기 좋게 dict 형식으로 변경\n",
    "idx2word = dict(enumerate(results))\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairly'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[396]\n",
    "idx2word[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반대로 문자를 넣으면 인덱스를 알려주는 것\n",
    "word2idx = {v:k for k, v in idx2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"bread\"]\n",
    "word2idx[\"tournament\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_paragraph(paragraph):\n",
    "    words = paragraph.split(\" \")\n",
    "    encoded = []\n",
    "    for word in words:\n",
    "        encoded.append([word2idx[word]])\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def encode_category(category):\n",
    "    if category == \"food\":\n",
    "        return [1, 0]\n",
    "    else:\n",
    "        return [0, 1]\n",
    "    \n",
    "def word_cnt(paragraph):\n",
    "    return len(paragraph.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"enc_paragraph\"] = df.paragraph.apply(encode_paragraph)\n",
    "df[\"enc_category\"] = df.category.apply(encode_category)\n",
    "df[\"seq_length\"] = df.paragraph.apply(word_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "      <th>enc_paragraph</th>\n",
       "      <th>enc_category</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[281], [248], [337], [501], [483], [299], [32...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[272], [469], [138], [202], [91], [308], [269...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[276], [201], [511], [220], [516], [235], [50...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[507], [113], [222], [513], [359], [151], [26...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[151], [231], [505], [93], [252], [382], [524...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[151], [206], [63], [167], [139], [85], [523]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[521], [283], [252], [372], [428], [210], [15...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[491], [428], [484], [185], [30], [423], [506...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[151], [304], [35], [96], [100], [128], [139]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[333], [151], [5], [365], [107], [440], [151]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[151], [417], [188], [519], [122], [378], [79...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[151], [211], [252], [59], [28], [526], [23],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[105], [250], [81], [191], [173], [197], [218...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[60], [118], [35], [362], [115], [43], [501],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[151], [274], [196], [492], [21], [184], [241...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[151], [106], [121], [352], [501], [472], [22...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[230], [478], [9], [141], [362], [463], [103]...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[146], [165], [503], [230], [478], [197], [39...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[342], [151], [306], [91], [196], [87], [362]...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[452], [35], [151], [518], [48], [268], [404]...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category  ... enc_category seq_length\n",
       "0   dishplace is located in sunnyvale downtown the...     food  ...       [1, 0]         53\n",
       "1   service can be slower during busy hours but ou...     food  ...       [1, 0]         19\n",
       "2   portions are huge both french toast and their ...     food  ...       [1, 0]         42\n",
       "3   we started with apps going the chicken and waf...     food  ...       [1, 0]         43\n",
       "4   the biscuits and gravy was too salty two peopl...     food  ...       [1, 0]         82\n",
       "5   the garlic fries were a great starter (and a h...     food  ...       [1, 0]         24\n",
       "6   our meal was excellent i had the pasta ai form...     food  ...       [1, 0]         50\n",
       "7   what i enjoy most about palo alto is so many r...     food  ...       [1, 0]         43\n",
       "8   the drinks came out fairly quickly a good two ...     food  ...       [1, 0]         49\n",
       "9   despite the not so good burger the service was...     food  ...       [1, 0]         82\n",
       "10  the four reigning major champions simona halep...   sports  ...       [0, 1]         65\n",
       "11  the briton was seeded nn7 here last year befor...   sports  ...       [0, 1]         88\n",
       "12  stephens surged her way back from injury in st...   sports  ...       [0, 1]         91\n",
       "13  when it came to england chances in the world c...   sports  ...       [0, 1]         71\n",
       "14  the team that eliminated russia – croatia – al...   sports  ...       [0, 1]         70\n",
       "15  the perseyside outfit finished in fourth place...   sports  ...       [0, 1]         30\n",
       "16  liverpool fc will return to premier league act...   sports  ...       [0, 1]         35\n",
       "17  alisson signed for liverpool fc from as roma t...   sports  ...       [0, 1]         30\n",
       "18  but the rankings during that run-in to new yor...   sports  ...       [0, 1]         63\n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports  ...       [0, 1]         46\n",
       "\n",
       "[20 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "# dynamic_rnn  : 불규칙 길이를 입력값을 동적으로 처리할 수 있게 함.\n",
    "\n",
    "# static_rnn : 같은 길이 입력값만 받음\n",
    "\n",
    "# 데이터의 전체길이를 아는게 중요하다. \n",
    "\n",
    "\n",
    "# 문장의 최저길이를 알아내기\n",
    "max_word_cnt = 0\n",
    "for row in df[\"paragraph\"]:\n",
    "    if len(row.split(\" \")) > max_word_cnt:\n",
    "        max_word_cnt = len(row.split(\" \"))\n",
    "        \n",
    "print(max_word_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    한줄씩 비교를 해서 큰값을 남기고 계속 차례로 비교해서 남기면\n",
    "    \n",
    "    제일 큰 값이 남게 되게 하는 간단한 알고리즘이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(enc_paragraph):\n",
    "    seq_length = len(enc_paragraph)\n",
    "    for i in range(seq_length, max_word_cnt):\n",
    "        enc_paragraph.append([-1])\n",
    "        \n",
    "    return enc_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"enc_paragraph\"] = df.enc_paragraph.apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151], [417], [188], [519], [122], [378], [79], [58], [477], [117], [42], [505], [361], [50], [348], [32], [301], [105], [123], [324], [139], [515], [503], [439], [151], [265], [185], [171], [362], [20], [161], [394], [371], [342], [105], [353], [41], [54], [151], [45], [528], [226], [151], [23], [228], [199], [394], [209], [249], [503], [36], [158], [350], [504], [297], [209], [125], [298], [151], [418], [417], [459], [501], [151], [514], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"enc_paragraph\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값들을 배열로 변환\n",
    "enc_paragraph = np.array(df.enc_paragraph.tolist())\n",
    "enc_category = np.array(df.enc_category.tolist())\n",
    "seq_length = np.array(df.seq_length.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = enc_paragraph\n",
    "train_y = enc_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 91, 1)\n",
      "(20, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-71-fd029f583faa>:12: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 모델구축\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, max_word_cnt, 1])\n",
    "y = tf.placeholder(tf.int32, [None, 2])\n",
    "\n",
    "embedding = tf.layers.dense(X, 5)\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64)\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedding, dtype=tf.float32, sequence_length=seq_length)\n",
    "\n",
    "dense_layer = tf.layers.dense(state.h, 32)\n",
    "logits = tf.layers.dense(dense_layer, 2)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.86892205, accuracy:0.5\n",
      "epoch:50, loss:0.39301464, accuracy:0.9\n",
      "epoch:100, loss:0.055023037, accuracy:1.0\n",
      "epoch:150, loss:0.0067080595, accuracy:1.0\n",
      "epoch:200, loss:0.0021789588, accuracy:1.0\n",
      "epoch:250, loss:0.001160603, accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        _, cost = sess.run([train, loss], feed_dict={X:train_X, y:train_y})\n",
    "        if epoch % 50 == 0:\n",
    "            pred = tf.nn.softmax(logits)\n",
    "            correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "            cur_acc = accuracy.eval({X:train_X, y:train_y})\n",
    "            print(\"epoch:\" + str(epoch) + \", loss:\" + str(cost) + \", accuracy:\" + str(cur_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
