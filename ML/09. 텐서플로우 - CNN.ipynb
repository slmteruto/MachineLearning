{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tf1) 가상머신에서 jupyter 실행할 것\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29f58037108>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3*3*1*1 이미지 준비 \n",
    "\n",
    "# 2*2*1 필터 준비\n",
    "image = np.array([[[[1], [2], [3]],\n",
    "                 [[4], [5], [6]],\n",
    "                 [[7], [8], [9]]]], dtype=np.float32)\n",
    "\n",
    "\n",
    "image.shape\n",
    "\n",
    "# 이미지를 2차원크기로 보내야 한다.\n",
    "# 근데 이미지는 4차원이니까 reshape으로 줄여야함\n",
    "plt.imshow(image.reshape(3,3), cmap=\"Greys\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩없이  Convolution layer 추출\n",
    "\n",
    "filter = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2), Dimension(1), Dimension(1)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "축의 방향을 바꿈: [[[[12.]\n",
      "   [16.]]\n",
      "\n",
      "  [[24.]\n",
      "   [28.]]]]\n",
      "차원 변경: [[12. 16.]\n",
      " [24. 28.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorflow에 conv2d   : convolution layer를 추출해주는 함수\n",
    "\n",
    "conv2d = tf.nn.conv2d(image, filter, strides=[1,1,1,1], padding =\"VALID\")  \n",
    "# 가로 세로 한칸씩 이동이라는 뜻\n",
    "# [1,1,1,1] 에서 첫번째 네번째 1은 의미없는 자리이다 그냥 자리 채우기용\n",
    "\n",
    "# [1,2,2,1] 이라고 하면  가로 세로로 두칸씩 이동이라는 것\n",
    "\n",
    "sess = tf.Session()\n",
    "conv2d_img = sess.run(conv2d)\n",
    "\n",
    "print(conv2d_img.shape) # 1, 2, 2, 1로 크기가 작아짐을 알 수 있음.\n",
    "\n",
    "\n",
    "# 특징을 뽑아낸 것들 그래프 \n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"축의 방향을 바꿈:\", conv2d_img)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(\"차원 변경:\", one_img.reshape(2, 2))\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.imshow(one_img.reshape(2,2), cmap=\"Greys\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## padding을 이용한 convolution layer 추출\n",
    "filter = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])\n",
    "\n",
    "\n",
    "conv2d = tf.nn.conv2d(image, filter, strides=[1,1,1,1], padding =\"SAME\")  \n",
    "# padding을 SAME을 하면 같은 모양을 그대로 유지할 수 있게 한다. \n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "conv2d_img = sess.run(conv2d)\n",
    "\n",
    "print(conv2d_img.shape) # 1, 2, 2, 1로 크기가 작아짐을 알 수 있음.\n",
    "\n",
    "\n",
    "# 특징을 뽑아낸 것들 그래프 \n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"축의 방향을 바꿈:\", conv2d_img)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(\"차원 변경:\", one_img.reshape(2, 2))\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.imshow(one_img.reshape(2,2), cmap=\"Greys\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 3)\n",
      "축의 방향을 바꿈: [[[[ 12.]\n",
      "   [ 16.]\n",
      "   [  9.]]\n",
      "\n",
      "  [[ 24.]\n",
      "   [ 28.]\n",
      "   [ 15.]]\n",
      "\n",
      "  [[ 15.]\n",
      "   [ 17.]\n",
      "   [  9.]]]\n",
      "\n",
      "\n",
      " [[[120.]\n",
      "   [160.]\n",
      "   [ 90.]]\n",
      "\n",
      "  [[240.]\n",
      "   [280.]\n",
      "   [150.]]\n",
      "\n",
      "  [[150.]\n",
      "   [170.]\n",
      "   [ 90.]]]\n",
      "\n",
      "\n",
      " [[[-12.]\n",
      "   [-16.]\n",
      "   [ -9.]]\n",
      "\n",
      "  [[-24.]\n",
      "   [-28.]\n",
      "   [-15.]]\n",
      "\n",
      "  [[-15.]\n",
      "   [-17.]\n",
      "   [ -9.]]]]\n",
      "차원 변경: [[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n",
      "차원 변경: [[120. 160.  90.]\n",
      " [240. 280. 150.]\n",
      " [150. 170.  90.]]\n",
      "차원 변경: [[-12. -16.  -9.]\n",
      " [-24. -28. -15.]\n",
      " [-15. -17.  -9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 3개의 필터 사용해보기   (2*2*1*3)\n",
    "## 그러면 세개의 이미지가 생성된다.\n",
    "\n",
    "\n",
    "filter = tf.constant([[[[1., 10, -1]],[[1., 10, -1]]],\n",
    "                      [[[1., 10, -1]],[[1., 10, -1]]]])\n",
    "\n",
    "\n",
    "conv2d = tf.nn.conv2d(image, filter, strides=[1,1,1,1], padding =\"SAME\")  \n",
    "# padding을 SAME을 하면 같은 모양을 그대로 유지할 수 있게 한다. \n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "conv2d_img = sess.run(conv2d)\n",
    "\n",
    "print(conv2d_img.shape) # 1, 2, 2, 1로 크기가 작아짐을 알 수 있음.\n",
    "\n",
    "\n",
    "# 특징을 뽑아낸 것들 그래프 \n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(\"축의 방향을 바꿈:\", conv2d_img)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(\"차원 변경:\", one_img.reshape(3, 3))\n",
    "    \n",
    "    #이미지가 세장이 나올것이니까 1행3열로 바꿔준다.\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(one_img.reshape(3,3), cmap=\"Greys\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    필터로 만들어진 이미지 순서대로 이다.\n",
    "    패딩을 했기 때문에 사이즈가 같아짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n",
      "[[[[4]]]]\n"
     ]
    }
   ],
   "source": [
    "#### Max pooling (2*2)\n",
    "image2 = tf.constant([[[[4],[3]],\n",
    "                     [[2], [1]]]])\n",
    "\n",
    "# 2*2 의 커널이 만들어짐?\n",
    "pool = tf.nn.max_pool(image2, ksize=[1,2,2,1], strides=[1,1,1,1], padding=\"VALID\")  \n",
    "\n",
    "# 세션\n",
    "\n",
    "sess = tf.Session()\n",
    "p = sess.run(pool)\n",
    "print(p.shape)\n",
    "print(p)\n",
    "\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 실습예제_MNIST\n",
    "\n",
    "    https://docs.google.com/presentation/d/1h90rpyWiVlwkuCtMgTLfAVKIiqJrFunnKR7dqPNtI6I/edit#slide=id.g1ee4a504dd_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-3ce75967a7a4>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist.train.images[0]\n",
    "img.shape # (784,)\n",
    "\n",
    "plt.imshow(img.reshape(28, 28), cmap=\"gray\") # 2차원으로만 들어가니까 1차원 (784,) → 2차원 28*28\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫번째 Convolution Layer 준비\n",
    "\n",
    "    Convolution1 ->  ReLU1  ->  Max pool1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차원 MNIST 데이터 4차원으로 Reshape\n",
    "## tf.reshape(X, [이미지갯수, 가로, 세로, 색])\n",
    "## 이미지갯수 -1 = None\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1]) # 이미지갯수 미지정의 가로 28, 세로 28, 흑백(1) 이미지로 Reshpae\n",
    "\n",
    "\n",
    "# 필터(크기 3*3 / 갯수 32개 / 색상 수 1) 준비\n",
    "#    * 필터사이즈 & 스트라이드 사이즈는 조절해가면서 성능 조절~~~~~\n",
    "# Convolution layer는 가중치로 뽑는거니까\n",
    "# W = tf.Variable(tf.random_normal(가로, 세로, 색상, 갯수), stddev=이게머라구?)로 준비하긔\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Convolution Layer\n",
    "## L1 = tf.nn.conv2d(이미지데이터, 필터, strides=[디퐅트1, 가로1, 세로1, 디폴트1], padding=\"SAME\")\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "print(L1)\n",
    "\n",
    "L1 = tf.nn.relu(L1)\n",
    "print(L1)\n",
    "\n",
    "# Convolution Layer으로 뽑은 이미지에서 Max Pooling Layer\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "print(L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    최초 컨볼루션할 때 모양은 28* 28이다.\n",
    "    \n",
    "    그것을 Max pool로 뽑아 내었을 때 크기는 14*14가 된다.\n",
    "    \n",
    "    원래는 더 작아졌을 텐데 padding = same을 해서 14*14의 크기가 된것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 두번째 Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터(크기 3*3 / 갯수 64개 / 색상 수 1) 준비\n",
    "# W = tf.Variable(tf.random_normal(가로, 세로, 색상, 갯수), stddev=범위)로 준비하긔\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "# 아니 3번째꺼 색상이라면서요 근데 왜 갖고온 갯수가 32개니까 3번째에 32개 넣는다고 하시는데요;;;;;\n",
    "# 아 처음 받을때는 색상 수인데 두번째에서는 이전꺼 갯수로 받는다고요\n",
    "# 왜 그러는건데...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Convolution Layer\n",
    "## L1 = tf.nn.conv2d(첫번째 Convolution layer, 2번째필터, strides=[디퐅트1, 가로1, 세로1, 디폴트1], padding=\"SAME\")\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "print(L2)\n",
    "\n",
    "L2 = tf.nn.relu(L2)\n",
    "print(L2)\n",
    "\n",
    "# Convolution Layer으로 뽑은 이미지에서 Max Pooling Layer\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "print(L2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    패딩이 SAME이지만 strides = [1, 1]에서 [2, 2]로 올라서 \n",
    "    \n",
    "    14*14에서 7*7로 크기가 줄어든 것이다. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected Layer 작성\n",
    "\n",
    "    FC\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 준비\n",
    "learning_rate = 0.001\n",
    "training_epochs = 30\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-60aa80ac6faf>:12: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-60aa80ac6faf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_xy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xy\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########## Tensor graph 작성\n",
    "\n",
    "# 입력데이터 준비 (L를 4차원에서 2차원으로) / 정답(y)는 위에서 작업해놨구요\n",
    "L2 = tf.reshape(L2, [-1, 7*7*64])\n",
    "\n",
    "# 가설준비\n",
    "W3 = tf.Variable(tf.random_normal([7*7*64, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "### 비용 함수 ###\n",
    "logit = tf.matmul(L2, W3) + b\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "### 최소비용 계산 ###\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "########## Tensor graph 실행\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_xy = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_xy})\n",
    "        avg_cost += c/total_batch\n",
    "    \n",
    "#     print(\"Epoch :\", \"%04d\"%(epoch+1), \"cost :\", \"{:.9f}\".format(avg_cost))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9855\n"
     ]
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "correct_pred = tf.equal(tf.argmax(logit, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### CNN 실습예제_교통표지판\n",
    "\n",
    "\n",
    "    http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset\n",
    "    GTSRB_Final_Test_Images.zip\n",
    "    GTSRB_Final_Training_Images.zip 다운로드\n",
    "    \n",
    "#### Image format\n",
    "    Images are stored in PPM format (Portable Pixmap, P6)\n",
    "\n",
    "    Image sizes vary between 15x15 to 250x250 pixels\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지전처리 준비\n",
    "    \n",
    "    Convolution Layer1 → Max Pooling → Convolution Layer2 →Max Pooling →FC(Fully Connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # 동시에 여러 폴더(or)파일을 불러오기 위한 패키지\n",
    "from skimage.color import rgb2lab # RGB를 간단한 색상으로 바꿔주는 패키지\n",
    "from skimage.transform import resize # 이미지 사이즈 변경해주는 패키지\n",
    "from collections import namedtuple # 뭐욤?\n",
    "\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절대 변하면 안되는 상수 값들을 전부 고정값으로 표시해두기(ex. 표지판클래스 개수 43개)\n",
    "N_CLASS = 43\n",
    "RESIZED_IMAGE = (32, 32)\n",
    "\n",
    "Dataset = namedtuple(\"Dataset\", [\"X\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기 & 전처리\n",
    "# X데이터 차원 조정하는 함수\n",
    "def to_tf_format(imgs):\n",
    "    # imgs : 리스트형식 데이터 받아와찌\n",
    "    # np.stack() : 차원과 차원을 합쳐서 다차원으로 만들어주는 놈\n",
    "    return np.stack([img[:, :, np.newaxis] for img in imgs]).astype(np.float32)\n",
    "\n",
    "# read_dataset_ppm(경로, 클래스(폴더)개수, 조정할사이즈)\n",
    "def read_dataset_ppm(roofpath, n_labels, resize_to):\n",
    "    images, labels = [], []\n",
    "    \n",
    "    for c in range(n_labels):\n",
    "        full_path = roofpath + \"/\" + format(c, \"05d\") + \"/\" # 0~42(range(n_labels))의 값을 5자리로 양식화해 경로지정\n",
    "        \n",
    "        for img_name in glob.glob(full_path+\"*.ppm\"): # 위에서 만든 경로의 확장자가 ppm인 모든 파일 불러옴\n",
    "            # plt.imshow(숫자) : 숫자를 이미지화해서 보여줌\n",
    "            # plt.imread(이미지데이터) : 이미지를 숫자화해서 보여줌\n",
    "            img = plt.imread(img_name).astype(np.float32)\n",
    "            \n",
    "            img = rgb2lab(img/255.0)[:, :, 0] # [:, :, 0] 뭔소리야 이게\n",
    "            \n",
    "            if resize_to: # 이미지 사이즈 변환\n",
    "                img = resize(img, resize_to)\n",
    "            \n",
    "            # Labeling\n",
    "            ## one-hot encoding을 할껀데요\n",
    "            ## 43개 중에 1번째면 100000000.....\n",
    "            ## 43개 중에 2번째면 010000000.....\n",
    "            label = np.zeros((n_labels), dtype=np.float) # 일단 카테고리 43개에 0을 채워놓구요\n",
    "            label[c] = 1.0 # 각 카테고리 위치에 one-hot을 주는겁니다\n",
    "            \n",
    "            # 이제 각 변수에 값을 넣어줍시다\n",
    "            images.append(img.astype(np.float32))\n",
    "            labels.append(label)\n",
    "    \n",
    "    # 이후 작업에서 편하게 쓰기 위해서 차원도 미치 X는 4처원 y는 2차원으로 맞춰둡시다\n",
    "    return Dataset(X=to_tf_format(images), y=np.matrix(labels).astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset_ppm(\"GTSRB/Final_Training/Images\", N_CLASS, RESIZED_IMAGE)\n",
    "\n",
    "# 이 인자들이 (\"경로\", 43, (32,32))라면 나중 혹 타인이 43, (32,32) 숫자가 무슨 의미인지, 왜 그 값인지 알 기 어렵ㅇㅇ\n",
    "# 그래서 위에서 상수값을 변수로 고정한거\n",
    "\n",
    "# print(dataset.X.shape) # (39209, 32, 32, 1) 4차원~~~~~~\n",
    "# print(dataset.y.shape) # (39209, 43) 2차원~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAarElEQVR4nO2da4xd1XXH/+u+Zu6dh+3B2BjbvBIUBaEEoimiIorSpEQURSJUTZRUSqlK46gKUiOlHxCVGir1A6maRPlQUTmFhrRpgOahoAq1QSgViaqQOJRXQhsMMWA8eGzs8bxn7mP1w72og3v+a2bO3IfD/v+k0dzZ++5z1tl3r3vu7P9da5m7Qwjx1qcwaAOEEP1Bzi5EIsjZhUgEObsQiSBnFyIR5OxCJEJpK4PN7AYAXwFQBPD37n5X9PyKDfuwjWz+REwetM0fat2BgRRpBfLeWCrywwV9KOS8gEAttdbmpVTPPY+BHcSMVoXfX1rlfIYUF1vcjmYzsz16XWylnssOFIN7p23+2jw4ntWzr2upcQarzaXMk+V2djMrAvhbANcDOArgp2b2kLv/go0ZthFcW75h0+dy8oJZXmcxPonsXABQqA5nt0/soGOaO7fxvpEy7YsWR2Glsfm+4E3Mi8EbUk6sle2AS/tG6ZiF3fmW43nPzNO+4utzme2NnWN8zEvH+cmiN9Nt/NpQCV5rQmN7lfaVp2Yy2//z6D/RMVv5GH8NgMPu/qK7rwK4H8BNWzieEKKHbMXZ9wJ4Zc3fRzttQohzkK38z571OfP/fcYxswMADgDAMGpbOJ0QYits5c5+FMD+NX/vA3Ds7Ce5+0F3n3T3ybJl/88rhOg9W3H2nwK43MwuNbMKgI8DeKg7Zgkhuk3uj/Hu3jCz2wD8O9rS273u/vNojAEwIidE0XeGHLvFOXfqLdChfGUls7352jQ348ws7StObKd9jQt43+qOIdrXKmfv4Howha0Sv+aob3kH72vUsvvK8/x1Hn+ZqwzVwydpnx8Lds/Hs3fdvcRVEguUEG8EshyTZgG88Pvn0b76/ux1dfOVT9IxP77rNzLbm6f5rv+WdHZ3fxjAw1s5hhCiP+gbdEIkgpxdiESQswuRCHJ2IRJBzi5EImxpN76bWBSMYdlBFaFcF8knOZNs0oCcwPTm/ALts8VF2leY5lJTlQTkAIBtG89sr+8JZL5qEAEWTNXYS1yGKp4m1zbDpcjwNasHkhcJugGA1lx2kEz51VP8eMNc2nRyPACwIJKuPsYn8sXr781s//EyD8r6/qXXZrY3uem6swuRCnJ2IRJBzi5EIsjZhUgEObsQidDf3XgzHiwQ5egiY1jqIyD/jnuUsiradadjynyKWZorALDtPFDDoxRHS9lBFeWX+e5+aSg4XqSSRMFGLL3XGM9BuLKPp/ca+tUJ2udLy5u3I8CD1yzkxGnatfvHE7Tvve/43cz21+f4XI0dz17fhShOh3cJId5KyNmFSAQ5uxCJIGcXIhHk7EIkgpxdiEQ4ZwJhovxdLEDCV1f58aLKHXkryRCsyit32O6dtM9rQcBFcL6lvbyayYmrsmW05d1BsMhQVD6Jz5Wt8r7KbPbrOfIqv7LqSW5HeYZfsy1y6c2Xs/uau3hg0Gu/yc914QM8EAYNnkNv9Fi2JAoAC/+wK7N9Z53P1egvs2W+0mJQ0Yj2CCHeUsjZhUgEObsQiSBnFyIR5OxCJIKcXYhE2JL0ZmZHAMwBaAJouPtk9Hx353nconFR5BUjkNeiXGcoB/nYxkYz2/18HtHUrPGIsoX9vKrt8WuC9+H9S7wP2WFPpWIgvTUj2ZPLP6Uyl3larew5Pv12vuROT3MpcvxCLpWd/2SF9pVffC2zfX43l0tn386v68Igas9IxCEAeLAep67Pluxsgc/VO17YvHzcDZ39t9ydx08KIc4J9DFeiETYqrM7gO+b2c/M7EA3DBJC9Iatfoy/zt2PmdkuAI+Y2X+7+2Nrn9B5EzgAAMPg/6MKIXrLlu7s7n6s83sawHcBXJPxnIPuPunuk2XjaZiEEL0lt7Ob2YiZjb3xGMCHADzbLcOEEN1lKx/jdwP4bkfGKgH4Z3f/t2iAYR3ZixEklsxFEGFXmOBJD1s7s5NANsa4ZHTqCv5p5tTVXOIZmgjKDAVTyKSyZk55rRBIdo06lynZuMoQjwxbDmTKVonb76WgbNRE9ms28uIMHXPJQ0GE3Twv2eWLXBKdeTtfI/d94O7M9j/+yR/QMa1q9lxFEl9uZ3f3FwG8O+94IUR/kfQmRCLI2YVIBDm7EIkgZxciEeTsQiRCXxNOOngNtlCSy1u3jWA1HvHk27Ij2wBgZWf2uJnLedTVzJVcuiqO84SZhQK/ZhZRBsSyHB+U71zROPfscSsLXF6rvcSX4+gxLlOujvNjWj1b+iyd4Ukqh15boH1e58XUovqCpUXe94c//KPM9tGnuGxbnM2OPbMmX2+6swuRCHJ2IRJBzi5EIsjZhUgEObsQidDX3fgwEIbkpgMAsBx0OUpGAQAmeD6z+o4gN9nF2bvus5fxU1m44853TqPAlUicYLvgUbALG9PuzFcqa3Upe2kN/yrIM3ckUC5Wuf3W5H2NGlniHux0z/FccjYchGmXuDtZEMtVOZJ9zL2PZpd4AgAcO57dXueBRrqzC5EIcnYhEkHOLkQiyNmFSAQ5uxCJIGcXIhH6Kr3BjMtlUZ65SJYjFLZn5x4DgFaNyz+Le4K+C7NlqOZ2HhxRLuXLnxfJa3lihiJ5zVv53vObDT6u/Fq2TDn2Mr+w8mIgva0EwTplfm2tcraNTSbJrdNXDPINlo+foX2143yNvP6u7Lla2seDsqqHicQWLBzd2YVIBDm7EIkgZxciEeTsQiSCnF2IRJCzC5EI60pvZnYvgA8DmHb3KzttEwAeAHAJgCMAPubuQYhOB3cqsUX5uxiFKo9A8lFeMba+nY9b3MVLGi1fQCTAQF6L5KlWUD4J0XREfU0iQ+V9Ww9y4ZVO8txvY0ey22sneFRWRCso8WQtbmNzOPvCZy/hS78+xs81dJrnGzwvyP82NDVL+8YP78xsLy0GknMQ8UmHbOA5XwNww1lttwN41N0vB/Bo528hxDnMus7eqbd+6qzmmwDc13l8H4CPdNkuIUSXyfvhbre7TwFA5/eu7pkkhOgFPf+6rJkdAHAAAIZtpNenE0IQ8t7Zj5vZHgDo/J5mT3T3g+4+6e6TFfDvFQshekteZ38IwC2dx7cA+F53zBFC9IqNSG/fBPB+ADvN7CiAzwO4C8CDZnYrgJcBfLSXRtLkkUGCvyg0rD7Gx62O80M6kdiKp4KSRq/y99PadBARF0W95ZDl6jU+Hys7eF8zyK9YPcENGXmNyEZRssxiIK8FSSVbFT5u4QIivQVJQptjXB5svsLXzsgunqx05DQvKbXrh+SD8Ymz98X/D88RCbqus7v7J0jXBzd9NiHEwNA36IRIBDm7EIkgZxciEeTsQiSCnF2IROhrwklHvug2VuvNAunNK1wOW9nG3+PqY5E2lC3xlBa49FM5w49XmcuXjBJR+TVyukJQO64UJHMsBEFq5XluP5XKgtuLNQJ5bYhf9NIEjx6cv5gcbzev51YItM1mlZ8rWle1IMmpHTuZ2e4r3MY86M4uRCLI2YVIBDm7EIkgZxciEeTsQiSCnF2IROhvrbcAI/IaAJ5cL5Leyvx4jeGgNthQ3nCzrg0BABQCGcoLQd02MlXlBR4lVT2x+QgqAGjUgteMESmbQVLJlTF+riiCrb57NbO9WAzmN1BEGzU+bmUHv3c2R3iiynIp+9o8T1G/AN3ZhUgEObsQiSBnFyIR5OxCJIKcXYhEOGd246OccZQgD1ezxgNhmkFQRasU7HKSt0YSH9M5Hu+LAlqiHfeo3FFhNbtv6PXl4GRRAAq/gEKQM65VyZ6sSGVoBte8ui3fznThTPY6aI7wtWOVQJ0I1JoWX3JoDQXKBVGVrByoTfXNl9HSnV2IRJCzC5EIcnYhEkHOLkQiyNmFSAQ5uxCJsJHyT/cC+DCAaXe/stN2J4BPATjRedod7v7wBo7FA16iL/a3SGRCMMYaQX60QLoKI1dIlweqStQXlngKbLRIGTqVnbescGaRmzHEgzS8yvWkqFxTYTV7/r3E7y+zF/PluLCPz0ejmiPaqBkEE7UCmS8IoGnyaUQzlN5yBBQx2TmSITdw2K8BuCGj/cvuflXnZ11HF0IMlnWd3d0fA8ArzAkhfi3Yyv/st5nZ02Z2r5nt6JpFQoiekNfZ7wbwNgBXAZgC8EX2RDM7YGaHzOzQqgdf2RRC9JRczu7ux9296e4tAF8FcE3w3IPuPunukxULin0LIXpKLmc3sz1r/rwZwLPdMUcI0Ss2Ir19E8D7Aew0s6MAPg/g/WZ2Fdri0REAn97IydwdTiQDi6LeytnyT1RKKlTQ8kTYATxKLTicBfnMwr5AXquc4v8OFWaXso+3FJQSYjn+ABRWuSHlYI6Z1HTmokBe28sPWB8PotSCsMPCIrm2VS53+RKfDw+iIln+v/X6qLQcScQ5WNfZ3f0TGc33dNUKIUTP0TfohEgEObsQiSBnFyIR5OxCJIKcXYhE6G/CSXf4anY5HlR4yBBVvHJKaFHSw8IqP2azSjpyKiTW5ANLCzyhYHGBzCEAmyfRbTnnyuqBPhhQJLLR2FF+f6mdDCSvnPZ7Idv+5aBU09Ju3reyg89Hoc7tKK5EOiu5tqjMF5Gdo6WoO7sQiSBnFyIR5OxCJIKcXYhEkLMLkQhydiESof+13iz7/SWPjOZ1rnWw6C8AKC+M0L7iSiC9MTuCfIGNanC8If5eWzmdT/LycXJtTX48Hw6KlAUJIr3I+5rVzS+t4nIkT/GuSJYzIlE1qsF9LjAjkmaLXBFFIUiAipXsgXnquUXozi5EIsjZhUgEObsQiSBnFyIR5OxCJEJ/d+MNsODL/RRW6qYe7MIu8N346km+yzm3wANyVhvZ54vykhXIGAAoz3E7Cqu8r1XjNhZmFjLbLdiNxxLfRq7vYNE/wNxFQ7SvQYYF6eLQHOadrWClRvkGacmu4HiNkeD1DHIDlueDwKYzQQ5AlksxLIm2+egr3dmFSAQ5uxCJIGcXIhHk7EIkgpxdiESQswuRCBsp/7QfwNcBXIB2iMBBd/+KmU0AeADAJWiXgPqYu58OD+aAM8mgsfkv/bNSUgBQmOcyztBJLstVZristTKR/d4YBUcMn+KSV+X1oIzTIpdqvMxfNh8hxTNXeNBQY4IHBp1+B5fXlnYFUhmpDRVJaPWJYA2UA+mwzu9Z5ZnsKKVCELQSUVzk11yZC9bjLMkNCMDJ2o/WtzfI6xnIdRu5szcAfM7d3wngWgCfMbMrANwO4FF3vxzAo52/hRDnKOs6u7tPufsTncdzAJ4DsBfATQDu6zztPgAf6ZWRQoits6n/2c3sEgBXA3gcwG53nwLabwgAdnXbOCFE99iws5vZKIBvA/isu89uYtwBMztkZofqCL4yKIToKRtydjMro+3o33D373Saj5vZnk7/HgDTWWPd/aC7T7r7ZBl8s0cI0VvWdXZr54u6B8Bz7v6lNV0PAbil8/gWAN/rvnlCiG6xkai36wB8EsAzZvZkp+0OAHcBeNDMbgXwMoCPbuiMni2heBBNZEWS5C2SJpb5vwzF4zO0b3RqlPY1akTGCRSjSI5pBXnamqNcAvRSEO1HSls1dtfomDOX8Rx0s5fRLjTHggsvEAmItQNAMYryCsLlAlmuvp10BHYUFnlSweo0t6N6Mqj/tMjlXic56KL1zXI5Rqzr7O7+I/B0fx/c9BmFEANB36ATIhHk7EIkgpxdiESQswuRCHJ2IRLhnCn/xCS5dtfmk+uF5Y4WeATS6AtnaF9llkSHBeYV6tyORi3f9EcJFhvj2bLRmYsDee1ybmNrO5eToopdzqSywPbweLwrLA2FCrm2VX6fiyLbhk5zSyrT2ck+AcCXeYRjJBMzqBwdSJS6swuRCHJ2IRJBzi5EIsjZhUgEObsQiSBnFyIR+i+9tUgkT6zjkDH53qt8iUcg2auZYfkAgOH58cz21jiPKGuM8Rj+YiDLtUr82poV3rewO/slXbiIS0atWhBdFRRni0qR8UFBVzNHHUCgnQaV9mUfs7DM57B6nNsx/hKX0GxmjpsRyGteJ1FvgU8YDczbWsJJIcRbADm7EIkgZxciEeTsQiSCnF2IROjvbrwZrExyqxWCXd/65ktDRXgQJIMoKMGyd1utxHOWFYMd1eY4zzMX7bgvnc9ftrmLs89X3x6VVoqiU/JsuUcEO+7RbnwpsCMYZyvZ81h7lc/vjud58E9limdR9yDPXGs1yE9H1ohV+PoADQ5TIIwQySNnFyIR5OxCJIKcXYhEkLMLkQhydiESYV3pzcz2A/g6gAvQDjk46O5fMbM7AXwKwInOU+9w94fjozkPagliMfIEwnhQOscimW8lkN4K2eezMzwAohAF+ARvtUs7uewyt58fc3Unue6otFJEEAgTwiS76HhRaahGICktcOmzdix7ks/7BZfCar/i5cFwiucojAKsQnIGdG2WjejsDQCfc/cnzGwMwM/M7JFO35fd/W96Z54QoltspNbbFICpzuM5M3sOwN5eGyaE6C6b+vxgZpcAuBrA452m28zsaTO718x2dNk2IUQX2bCzm9kogG8D+Ky7zwK4G8DbAFyF9p3/i2TcATM7ZGaH6r75/NhCiO6wIWc3szLajv4Nd/8OALj7cXdvunsLwFcBXJM11t0Puvuku0+WjWdtEUL0lnWd3cwMwD0AnnP3L61p37PmaTcDeLb75gkhusVGduOvA/BJAM+Y2ZOdtjsAfMLMrkI76dURAJ9e/1BGZYZIKjtX8NXsXGEW5Qojch0AFIJoOQ/ehguhTLnJdiB/ZFtQaiiuyUSo84uuvM7navwFfsjxl7P/dRya4nIpTnLprTXLo96Qcw1HUjAlx5iN7Mb/CNmv3DqauhDiXELfoBMiEeTsQiSCnF2IRJCzC5EIcnYhEqGvCSfNDFYmp8wTieZBZFspuLQil3FC+YT0NecX6JACkesAwOo88mpbkGSzNp1dhgoA5vZlf3Fp8YIyHbO8k0tvjVo+Wa5ALq06ze8vo6/wRKAjU/zbl6V5PseFM4vZHaej6DVe4ilaH6F8nCeyLTpetIYJurMLkQhydiESQc4uRCLI2YVIBDm7EIkgZxciEfpb680daAV11hgFIjO0ckbK0TpZiJNYNrL1JAtkkKiunAeJKi2QfyoLPLHhxMmxzPbtVS69eSB75g2Ia4xkn88afD6KS1yKLCxyeQ0nTvE+UmMtTEgavJ6taO14NFnBOmiRNccSrQJ8DQc26M4uRCLI2YVIBDm7EIkgZxciEeTsQiSCnF2IROiv9NZtojpqecmRyM8DOcYKgeQSRTUFsovPzfNxo7XM5tWJbEkOAJpD/D3fmvzaIsmuUcs+5sp4MKaabTsAVE/y+dh2eJj2FQ4fze6I5j6SIiOZNXjNrBRIn8yWLteA051diESQswuRCHJ2IRJBzi5EIsjZhUiEdXfjzWwYwGMAhjrP/5a7f97MLgVwP4AJAE8A+KS7B9EK7QpELDAk7850P2E7sd7g+eJC01mADxDmGPOLLqR9x66fyGyffRfP4bZ9gpc0siASZrXBl0+LlIYaGeZLxBv8mk88v532lZb5Lv74K5Xsc7G8hgA8yP8XEgVR5VnfA9iNXwHwAXd/N9rlmW8ws2sBfAHAl939cgCnAdzaVcuEEF1lXWf3Nm8Iu+XOjwP4AIBvddrvA/CRnlgohOgKG63PXuxUcJ0G8AiAFwDMuPsbn3eOAtjbGxOFEN1gQ87u7k13vwrAPgDXAHhn1tOyxprZATM7ZGaH6h7k4xZC9JRN7QC4+wyA/wBwLYDtZvbGDs0+AMfImIPuPunuk2XjX2sUQvSWdZ3dzM43s+2dx1UAvw3gOQA/APB7nafdAuB7vTJSCLF1NhIIswfAfWZWRPvN4UF3/1cz+wWA+83srwD8F4B7NnRGFmSQJ51cmPMrJ3ny00USWqS95ZQUC/OkpBGAXU9UyblIO4CZd/Nzje3g51p8hQfXjL5EAmH4qdDgChpGuTqI4kowjznKJOUt8WRRCbMc6yr38QjrOru7Pw3g6oz2F9H+/10I8WuAvkEnRCLI2YVIBDm7EIkgZxciEeTsQiSCeS/kK3YysxMAXur8uRPAyb6dnCM73ozseDO/bnZc7O7nZ3X01dnfdGKzQ+4+OZCTyw7ZkaAd+hgvRCLI2YVIhEE6+8EBnnstsuPNyI4385axY2D/swsh+os+xguRCANxdjO7wcz+x8wOm9ntg7ChY8cRM3vGzJ40s0N9PO+9ZjZtZs+uaZsws0fM7PnO7x0DsuNOM3u1MydPmtmNfbBjv5n9wMyeM7Ofm9mfdtr7OieBHX2dEzMbNrOfmNlTHTv+stN+qZk93pmPB8wsO5smw937+gOgiHZaq8sAVAA8BeCKftvRseUIgJ0DOO/7ALwHwLNr2v4awO2dx7cD+MKA7LgTwJ/1eT72AHhP5/EYgF8CuKLfcxLY0dc5AWAARjuPywAeRzthzIMAPt5p/zsAf7KZ4w7izn4NgMPu/qK3U0/fD+CmAdgxMNz9MQCnzmq+Ce3EnUCfEngSO/qOu0+5+xOdx3NoJ0fZiz7PSWBHX/E2XU/yOghn3wvglTV/DzJZpQP4vpn9zMwODMiGN9jt7lNAe9EB2DVAW24zs6c7H/N7/u/EWszsErTzJzyOAc7JWXYAfZ6TXiR5HYSzZ6XfGJQkcJ27vwfA7wD4jJm9b0B2nEvcDeBtaNcImALwxX6d2MxGAXwbwGfdPchN03c7+j4nvoUkr4xBOPtRAPvX/E2TVfYadz/W+T0N4LsYbOad42a2BwA6v6cHYYS7H+8stBaAr6JPc2JmZbQd7Bvu/p1Oc9/nJMuOQc1J59ybTvLKGISz/xTA5Z2dxQqAjwN4qN9GmNmImY298RjAhwA8G4/qKQ+hnbgTGGACzzecq8PN6MOcmJmhncPwOXf/0pquvs4Js6Pfc9KzJK/92mE8a7fxRrR3Ol8A8OcDsuEytJWApwD8vJ92APgm2h8H62h/0rkVwHkAHgXwfOf3xIDs+EcAzwB4Gm1n29MHO96L9kfSpwE82fm5sd9zEtjR1zkB8C60k7g+jfYby1+sWbM/AXAYwL8AGNrMcfUNOiESQd+gEyIR5OxCJIKcXYhEkLMLkQhydiESQc4uRCLI2YVIBDm7EInwvwOpA+PRNDPDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset.X[-1, :, :, :].reshape(RESIZED_IMAGE))\n",
    "print(dataset.y[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련용 데이터 & 테스트 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "idx_train, idx_test = train_test_split(range(dataset.X.shape[0]), test_size=0.25, random_state=101)\n",
    "# 이전에는 데이터를 4개변수에 담았는데 지금은 2개에 인덱스를 담은 이유는\n",
    "# X는 4차원. y는 2차원이니까 4개변수에 담아오면 또 차원을 일일이 맞춰줘야하니꽈\n",
    "\n",
    "X_train = dataset.X[idx_train, :, :, :] # (29406, 32, 32, 1)\n",
    "X_test =  dataset.X[idx_test, :, :, :] # (9803, 32, 32, 1)\n",
    "y_train = dataset.y[idx_train, :] # (29406, 43)\n",
    "y_test = dataset.y[idx_test, :] # (9803, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 훈련 & 예측_함수로 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터를 원하는 크기로 나눠서 훈련할 수 있게 만드는 함수\n",
    "# minibatcher(훈련데이터, 훈련정답, 나눌사이즈, 데이터의 순서를 섞느냐 마느냐):\n",
    "def minibatcher(X, y, batch_size, shuffle):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    # → 예외처리 / 단위테스트 / 디버깅(개발중인)모드에서 이 조건이 맞지 않으면 에러를 띄움\n",
    "    # → 실제 배포시에는 지워줘야하는 코드\n",
    "    \n",
    "    n_samples = X.shape[0] # X의 갯수를 받아놓고요\n",
    "    \n",
    "    if shuffle: # 어차피 받아온 값이 불린이니까 연산자 안썼음 ㅇㅇ\n",
    "        idx = np.random.permutation(n_samples) # X갯수 받은거를 순서를 섞어서 인덱스 저장\n",
    "    else:\n",
    "        idx = list(range(n_samples)) # X갯수를 순서대로 인덱스 저장\n",
    "    \n",
    "    for k in range(int(np.ceil(n_samples/batch_size))): # 29406/10000을 반올림X, 올림으로 > range(3)\n",
    "        from_idx = k * batch_size\n",
    "        to_idx = (k+1) * batch_size\n",
    "        yield X[idx[from_idx:to_idx], :, :, :], y[idx[from_idx:to_idx], :] # 작업이 안정적으로 되도록 한거라는데 어떤 부분에서 안정적으로 처리한다는 건지 못들음 ㅇㅋ;;\n",
    "        # X, y데이터를 끊어서 작업해서 함수를 호출하는 순간 10000씩 끊어진다고... 머라고.... 왓....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 1) (10000, 43)\n",
      "(10000, 32, 32, 1) (10000, 43)\n",
      "(9406, 32, 32, 1) (9406, 43)\n"
     ]
    }
   ],
   "source": [
    "for nb in minibatcher(X_train, y_train, 10000, True):\n",
    "    print(nb[0].shape, nb[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected Layer\n",
    "    - W, b : 고정 값\n",
    "    - hypothesis, cost, train : 변동 가능\n",
    "        → 그니까 두개를 따로 모듈화합니다 ㅇㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가설 준비하는 함수\n",
    "def fc_nn_activation_layer(in_tensors, n_units):\n",
    "    W = tf.get_variable(\"fc_W\", [in_tensors.get_shape()[1], n_units], tf.float32, tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable(\"fc_b\",[n_units], tf.float32, tf.constant_initializer(0.0))\n",
    "    # → in_tensor는 3차원(?)이니까 크기를 갖고오기 위해서 in_tensor.get_shape()[1] 요렇게\n",
    "    # → n_units 요거는 걍 원래도 갯수를 가져올꺼라서 그냥 넣을거임 ㅇㅋ?\n",
    "    return tf.matmul(in_tensors, W) + b\n",
    "\n",
    "# 액티베이션 펑션 준비하는 함수\n",
    "def fc_layer(is_tensors, n_units):\n",
    "    return tf.nn.leaky_relu(fc_nn_activation_layer(is_tensors, n_units)) # 나중에 액티베이션 펑션을 바꾸고싶으면 리턴 부분만 바꿔주면 되니까~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution Layer\n",
    "\n",
    "    (convoultion layer → relu → Max Pooling → Drop Out)  이게 한 세트가 된다.  Dropdout은 주로 선택이고\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(in_tensors, kernel_size, n_units):\n",
    "    W = tf.get_variable(\"conv_W\", [kernel_size, kernel_size, in_tensors.get_shape()[3], n_units],\n",
    "                        tf.float32, tf.contrib.layers.xavier_initializer())\n",
    "    # [가로, 세로, 이전데이터에서입력받을갯수, 이미지갯수]\n",
    "    b = tf.get_variable(\"conv_b\", [n_units], tf.float32, tf.constant_initializer(0.0))\n",
    "    return tf.nn.leaky_relu(tf.nn.conv2d(in_tensors, W, [1, 1, 1, 1], \"SAME\"))\n",
    "\n",
    "\n",
    "def maxpool_layer(in_tensors, sampling):\n",
    "    return tf.nn.max_pool(in_tensors, [1, sampling, sampling, 1], [1, sampling, sampling, 1], \"SAME\")\n",
    "\n",
    "def dropout(in_tensors, keep_prob, is_training):\n",
    "    # is_training : True, False 값  True면 훈련중, False면 테스트 \n",
    "    return tf.cond(is_training, lambda:tf.nn.dropout(in_tensors, keep_prob), lambda:in_tensors)\n",
    "\n",
    "\n",
    "# cond(is_trianing, True, False)  로 넘겨줄 예정이다. \n",
    "# 첫번째 lambda는 True라면 실행  True라는게 훈련중이므로 dropout을 하기 때문에 keep_prob를 넘겨준다. \n",
    "# 두번째 lambda는 False라면 실행인데 False는 테스트 이므로 \n",
    "#      dropout을 실행시키지 말고 그냥 넘길것이기 때문에 in_tensors만 보냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary\n",
    "\n",
    "    위 코드들 요약\n",
    "\n",
    "    - convolution 1 (5*5, 32개 필터) 사용\n",
    "    - convolution 2 (5*5  64개 필터) 사용\n",
    "    \n",
    "    - FC : 출력 1024 개 unit을 사용\n",
    "    - Dropout : 60%만 가지고 훈련 (40%는 버리고)\n",
    "    - Activation Function : softmax\n",
    "    \n",
    "    \n",
    "    나중에 만들 때 이렇게 스펙을 정리해놔야 설명하기 좋다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 만든 코드들을 네트워크 구성\n",
    "\n",
    "\n",
    "def model(in_tensors, is_training) : \n",
    "    # ----1st conv layer : 5*5, 32 filter, 2x maxpool, 20% drop out\n",
    "    with tf.variable_scope(\"L1\") :\n",
    "        l1 = maxpool_layer(conv_layer(in_tensors, 5, 32), 2)\n",
    "        l1_out = dropout(l1, 0.8, is_training)\n",
    "        \n",
    "    # ---- 2nd conv layer : 5*5, 64 filter, 2x maxpool, 20% drop out\n",
    "    with tf.variable_scope(\"L2\") :\n",
    "        l2 = maxpool_layer(conv_layer(l1_out, 5, 64), 2)\n",
    "        l2_out = dropout(l2, 0.8, is_training)\n",
    "\n",
    "    # 평면화     \n",
    "    with tf.variable_scope(\"flatten\") :\n",
    "        l2_out_flat = tf.layers.flatten(l2_out)\n",
    "    \n",
    "    # FC : 1024 neurons, 40% drop dout\n",
    "    with tf.variable_scope(\"L3\") :\n",
    "        l3 = fc_layer(l2_out_flat, 1024)\n",
    "        l3_out = dropout(l3, 0.6, is_training)\n",
    "    \n",
    "    # output (출력계층)    \n",
    "    with tf.variable_scope(\"out\") :\n",
    "        out_tensors = fc_nn_activation_layer(l3_out, N_CLASS)\n",
    "        \n",
    "    return out_tensors\n",
    "# ↓↓↓↓↓↓↓↓ 설명 ↓↓↓↓↓↓↓↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [L1]\n",
    "    \n",
    "    with tf.variable_scope(\"L1\") :            \n",
    "            tf.variable_scope : L1이라는 이름으로 그룹화 하는 것이다. \n",
    "    \n",
    "        l1 = maxpool(conv_layer(in_tensors, 5, 32), 2)\n",
    "            \n",
    "            conv_layer : 우리가 만든 함수는 필터 크기가 [k, k]로 만들었으니까 한 값만 넘겨주면 된다.\n",
    "            maxpool : 위의 conv_layer의 결과값과 필터값을 매개변수로 한다. \n",
    "        \n",
    "        l1_out = dropout(l1, 0.8, is_training)\n",
    "            \n",
    "            우리가 만든 dropout 함수를 사용한다. \n",
    "            dropout : 매개변수는 maxpool한 값, 버릴 비율, is_training은 True False 여부\n",
    "            \n",
    "            \n",
    "    ----------------------------------------------------------\n",
    "    \n",
    "#### [L2]\n",
    "    \n",
    "    l2 방식도 변수만 달라지지 l1과 같은 방식이다. \n",
    "    \n",
    "    l2 = maxpool(conv_layer(l1_out, 5, 64), 2)\n",
    "                            ------\n",
    "        중요한 것은 conv_layer 부분에서 in_tensors를 받아오는게 아니라 \n",
    "        위에서 바꿔놓은 l1_out을 가져와야 한다는 점이다.\n",
    "\n",
    "\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "\n",
    "    여기까지가 전처리작업이다 \n",
    "    \n",
    "    4차원으로 작업했기 때문에  2차원으로 다시 변경해줘야 FC로 넘겨줄 수 있다. \n",
    "    \n",
    "    4차원 -> 2차원으로 하는 작업이 평면화라고 한다. \n",
    "    \n",
    "\n",
    "#### [평면화]\n",
    "\n",
    "    l2_out_flat = tf.layers.flatten(l2_out)\n",
    "        l2의 결과(4차원)를 flatten 함수에 넣어주기만 하면 평면화가 된다. \n",
    "\n",
    "\n",
    "#### [L3]\n",
    "\n",
    "    l3 = fc_layer(l2_out_flat, 1024)\n",
    "    l3_out = dropout(l3, 0.6, is_training)\n",
    "          \n",
    "          \n",
    "          1024인 이유는 \n",
    "          40%만 사용할 것이기 때문에 0.6을 버린다. \n",
    "      \n",
    "\n",
    "#### [최종 output]\n",
    "\n",
    "    out_tensors = fc_nn_activation_layer(l3_out, N_CLASS)  \n",
    "        사용하기 직전의 뭐, 몇개 출력\n",
    "        \n",
    "        그전까진 1024개로 이어왔는데 실제 출력의 갯수를 넘겨 받아야 하기 때문에 \n",
    "        43개를 넘겨 받아야한다.\n",
    "        \n",
    "        위에 상수로 만든 N_CLASSES를 사용한다. \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 우리가 실제로 사용할 함수\n",
    "def train_model(X_train, y_train, X_test, y_test, learning_rate, max_epochs, batch_size):\n",
    "    in_X_tensors_batch = tf.placeholder(tf.float32, shape=(None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1))\n",
    "    in_y_tensors_batch = tf.placeholder(tf.float32, shape=(None, N_CLASS))\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    logit = model(in_X_tensors_batch, is_training)\n",
    "    out_y_pred =  tf.nn.softmax(logit)\n",
    "    \n",
    "    # cost\n",
    "    loss_score = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=in_y_tensors_batch)\n",
    "    loss = tf.reduce_mean(loss_score)\n",
    "    \n",
    "    train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epochs in range(max_epochs) :\n",
    "        print(\"Epoch : \",epochs)\n",
    "        tf_score = []\n",
    "        \n",
    "        for mb in minibatcher(X_train, y_train, batch_size, True) : \n",
    "            tf_output = sess.run([train, loss], feed_dict={in_X_tensors_batch:mb[0], \n",
    "                                                         in_y_tensors_batch:mb[1],\n",
    "                                                        is_training:True})\n",
    "            \n",
    "            \n",
    "            tf_score.append(tf_output[1])\n",
    "            \n",
    "        print(\"training_loss_score : \", np.mean(tf_score))\n",
    "        \n",
    "    #TEST\n",
    "    print(\"TEST SET PERFORMANCE\")\n",
    "    y_test_pred, test_loss = sess.run([out_y_pred, loss], feed_dict={in_X_tensors_batch:X_test, \n",
    "                                                         in_y_tensors_batch:y_test,\n",
    "                                                        is_training:False})\n",
    "    \n",
    "    print(\"test_loss_score : \", test_loss)\n",
    "    y_test_pred_classified = np.argmax(y_test_pred, axis = 1).astype(np.int32)\n",
    "    y_test_true_classified = np.argmax(y_test, axis = 1).astype(np.int32)\n",
    "    \n",
    "    print(classification_report(y_test_true_classified, y_test_pred_classified))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    train_model(X_train, y_train, X_test, y_test, 0.001,)\n",
    "    \n",
    "        매개변수 : \n",
    "        훈련용 데이터, 테스트 데이터, learning_rate, 전체 epoch(반복횟수), 배치사이즈\n",
    "        \n",
    "        \n",
    "        - in_X_tensors_batch = tf.placeholder(tf.float32, shape=(None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1))\n",
    "            \n",
    "            X값이 될 것  (미리 지정할 수 없으니 placeholder)                        \n",
    "            shape\n",
    "                4차원이 될 것이기 때문에 shape이 4차원이 되어야 한다.\n",
    "                크기가 그런데 32*32쓰는건 상수로 만들어놨으니 RESIZED_IMAGE를 쓴다.\n",
    "                정사각형이면 RESIZED_IMAGE를 써도 되는데 이미지가 직사각형일 수도 있으니\n",
    "                배열 [0][1]번을 써준다. \n",
    "            \n",
    "            \n",
    "        - in_y_tensors_batch = tf.placeholder(tf.float32, shape=(None, N_CLASS))\n",
    "        \n",
    "            y값이 될 것 (미리 지정할 수 없어서 placeholder)\n",
    "            \n",
    "            shape\n",
    "                출력이 43개이므로 미리 만들어둔 상수값으로 써준다. \n",
    "            \n",
    "        - is_training = tf.placeholder(tf.bool)\n",
    "            training 여부 (이것도 미리 지정 아님)\n",
    "            bool값이 들어가므로 tf.bool을 이용한다. \n",
    "            \n",
    "        - logit = model(in_X_tensors_batch, is_training)\n",
    "            우리가 만든 model함수 사용\n",
    "            X값, training 여부를 넘겨준다. \n",
    "        \n",
    "        - out_y_pred =  tf.nn.softmax(logit)\n",
    "        \n",
    "        - loss_score = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=in_y_tensors_batch)\n",
    "        \n",
    "            v2가 권장사항이다. \n",
    "            평소에 cost라는 변수명으로 썼던 그것이다. \n",
    "            \n",
    "        - loss = tf.reduce_mean(loss_score)\n",
    "            loss_score에서 바로 평균내도 됐지만 너무 길어져서 따로 만듬\n",
    "        \n",
    "        \n",
    "        - train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        - for mb in minibatcher(X_train, y_train, batch_size, True) : \n",
    "            tf_output = sess.run([train, loss], feed_dict={in_X_tensors_batch:mb[0], \n",
    "                                                         in_y_tensors_batch:mb[1],\n",
    "                                                         is_training:True})\n",
    "            \n",
    "                in_X_tensors_batch[0] : 첫번째 값을 가져와야한다.\n",
    "                \n",
    "                is_training : 현재 훈련중이니까 True로 넘겨준다. \n",
    "        \n",
    "        \n",
    "            tf_score.append(tf_output[1])  \n",
    "                1번째가 cost이다. 평균을 내기위해 한 list로 모아둔다.\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0\n",
      "training_loss_score :  3.90862\n",
      "Epoch :  1\n",
      "training_loss_score :  0.6258551\n",
      "Epoch :  2\n",
      "training_loss_score :  0.31595212\n",
      "Epoch :  3\n",
      "training_loss_score :  0.20409803\n",
      "Epoch :  4\n",
      "training_loss_score :  0.15152648\n",
      "Epoch :  5\n",
      "training_loss_score :  0.11584053\n",
      "Epoch :  6\n",
      "training_loss_score :  0.09897056\n",
      "Epoch :  7\n",
      "training_loss_score :  0.07745737\n",
      "Epoch :  8\n",
      "training_loss_score :  0.07683411\n",
      "Epoch :  9\n",
      "training_loss_score :  0.06729703\n",
      "TEST SET PERFORMANCE\n",
      "test_loss_score :  0.05541778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88        67\n",
      "           1       0.94      0.99      0.97       539\n",
      "           2       0.98      0.99      0.99       558\n",
      "           3       0.98      0.98      0.98       364\n",
      "           4       1.00      0.99      0.99       487\n",
      "           5       0.98      0.97      0.98       479\n",
      "           6       1.00      0.97      0.99       105\n",
      "           7       0.99      0.99      0.99       364\n",
      "           8       1.00      0.99      0.99       340\n",
      "           9       0.99      1.00      0.99       384\n",
      "          10       1.00      0.98      0.99       513\n",
      "          11       0.99      0.99      0.99       334\n",
      "          12       1.00      0.99      1.00       545\n",
      "          13       1.00      1.00      1.00       537\n",
      "          14       0.99      1.00      1.00       213\n",
      "          15       0.99      0.99      0.99       164\n",
      "          16       1.00      1.00      1.00        98\n",
      "          17       1.00      1.00      1.00       281\n",
      "          18       0.99      1.00      0.99       286\n",
      "          19       1.00      0.98      0.99        56\n",
      "          20       1.00      0.95      0.97        78\n",
      "          21       0.98      1.00      0.99        95\n",
      "          22       1.00      1.00      1.00        97\n",
      "          23       0.98      1.00      0.99       123\n",
      "          24       0.99      1.00      0.99        77\n",
      "          25       0.98      1.00      0.99       401\n",
      "          26       0.98      1.00      0.99       135\n",
      "          27       0.97      0.95      0.96        60\n",
      "          28       1.00      0.99      1.00       123\n",
      "          29       0.99      1.00      0.99        69\n",
      "          30       1.00      0.95      0.97       115\n",
      "          31       0.99      0.99      0.99       178\n",
      "          32       0.93      1.00      0.96        55\n",
      "          33       1.00      0.99      1.00       177\n",
      "          34       0.99      0.99      0.99       103\n",
      "          35       1.00      0.98      0.99       277\n",
      "          36       1.00      1.00      1.00        78\n",
      "          37       1.00      0.98      0.99        63\n",
      "          38       1.00      0.99      1.00       540\n",
      "          39       1.00      1.00      1.00        60\n",
      "          40       0.96      0.96      0.96        85\n",
      "          41       1.00      1.00      1.00        47\n",
      "          42       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           0.99      9803\n",
      "   macro avg       0.99      0.98      0.99      9803\n",
      "weighted avg       0.99      0.99      0.99      9803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_model(X_train, y_train, X_test, y_test, 0.001, 10, 256)  #실제 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test_true_classified, y_test_pred_classified)\n",
    "\n",
    "# plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "# plt.colorbar()\n",
    "# plt.tight_layout()\n",
    "\n",
    "# netsong7 [11:40]\n",
    "# plt.imshow(np.log2(cm+1), interpolation=\"nearest\", cmap=plt.get_cmap(\"tab20\"))\n",
    "# plt.colorbar()\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
