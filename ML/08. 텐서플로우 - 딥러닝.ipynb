{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tf1) 가상머신에서 jupyter 실행할 것\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN\n",
    "    \n",
    "    \n",
    "    http://hunkim.github.io/ml/\n",
    "    Neural Network 1: XOR 문제와 학습방법, Backpropagation (1986 breakthrough) - 강의 자료\n",
    "    \n",
    "    Neural Network 1: XOR 문제와 학습방법, Backpropagation (1986 breakthrough) - 실습 자료\n",
    "\n",
    "    우리가텐서플로우로 설계를 하잖아\n",
    "    \n",
    "    만약에 처음에 설꼐된 그래프를 실행시키면 거슬러 올라가면서 그리기 시작한다. 이때 back-propagation이 자동으로 구해진다.\n",
    "    \n",
    "    \n",
    "    back-propagation로 인해 딥러닝의 문제가 해결되었다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR gate\n",
    "    \n",
    "    인공신경망 문제가 해결된 과정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1],\n",
    "                  [1], [1], [1], [1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"Weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32,name=\"bias\")\n",
    "\n",
    "#sigmoid  (2진분류니까)\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.37482592]\n",
      " [0.91826046]\n",
      " [0.9060477 ]\n",
      " [0.9944962 ]\n",
      " [0.8932644 ]\n",
      " [0.9926261 ]\n",
      " [0.9936633 ]\n",
      " [0.9996036 ]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.00799543]\n",
      " [0.04181385]\n",
      " [0.04142794]\n",
      " [0.18962592]\n",
      " [0.03672668]\n",
      " [0.1697416 ]\n",
      " [0.1711092 ]\n",
      " [0.52537465]] \n",
      "예측 :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "# 하나만 '거짓'이여도 0\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.7205515 ]\n",
      " [0.739375  ]\n",
      " [0.7387662 ]\n",
      " [0.7567768 ]\n",
      " [0.73581797]\n",
      " [0.75337815]\n",
      " [0.7539642 ]\n",
      " [0.77069414]] \n",
      "예측 :  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 :  0.75\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "# 서로 같은 값을 가질때만 0\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    계속 첫번째와 마지막을 못맞추고 있다.\n",
    "    W를 어떻게 학습해야하는가?\n",
    "    \n",
    "    그럼 사람이 찾아줘야한다는 것?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "x_data = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]] # 배열로 준비할 필요도 없음\n",
    "y_data = [0, 1, 1, 1, 1, 1, 1, 0] # 머신러닝에서는 1차원으로 넘겨주기\n",
    "\n",
    "clf = svm.SVC(C = 100)\n",
    "clf.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "examples = [[1, 1, 1], [1, 0, 1]]\n",
    "exam_labels = [0, 1]\n",
    "\n",
    "result = clf.predict(examples)\n",
    "print(result)\n",
    "\n",
    "score = metrics.accuracy_score(exam_labels, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    얘도 잘 골라 낸다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝을 이용한 XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.03591707]\n",
      " [0.995172  ]\n",
      " [0.9962648 ]\n",
      " [0.9876886 ]\n",
      " [0.997767  ]\n",
      " [0.9848238 ]\n",
      " [0.9816458 ]\n",
      " [0.03421855]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "# 안됐던 XOR 딥러닝 이용해보기\n",
    "# 그동안 하나만 가지고 했던 hidden 계층 여러개 사용\n",
    "# \n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# hidden layer1\n",
    "W1 = tf.Variable(tf.random_normal([3, 20]), tf.float32, name=\"weight\")\n",
    "    # 여기서 나오는 출력은 hidden layer2 로 넘어감\n",
    "    # 출력개수가 많을수록 학습량 증가, 시간 증가 \n",
    "b1 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "# hidden layer2\n",
    "W2 = tf.Variable(tf.random_normal([20, 1]), tf.float32, name=\"weight\")\n",
    "    # hidden layer1에서 출력 2개 시켜줬으므로 2를 입력 / hidden layer1과 2의 연결점 \n",
    "b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) # layer1 전달받음  \n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    중간에 계층을 하나 더 두어서 좀 더 깊게 분류하도록 한다.\n",
    "    \n",
    "    학습 횟수도 늘렸다.     \n",
    "    \n",
    "    그랬더니 안되었던 XOR Gate를 잘 분류하고 있다. \n",
    "    \n",
    "    아무 조절도 안하고 그랬는데도 정확도가 100이 나온다는것은\n",
    "    엄청난 확률이다. \n",
    "    \n",
    "    \n",
    "    Wide하다 : W의 shape에 y의 값이 많아지는 것. \n",
    "    Deep하다 : 계층이 더 많아 지는 것.   위에서는 두개이다. (W1, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위의 딥러닝 코드를 좀더 Wide하고 Deep하게 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.0073697 ]\n",
      " [0.9977407 ]\n",
      " [0.9974227 ]\n",
      " [0.9946171 ]\n",
      " [0.9988756 ]\n",
      " [0.9934971 ]\n",
      " [0.9932468 ]\n",
      " [0.01729864]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# hidden layer1\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "# hidden layer2\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias2\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) # layer1 전달받음  \n",
    "\n",
    "# hidden layer3\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias3\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W3) + b3) # layer1 전달받음  \n",
    "\n",
    "\n",
    "# hidden layer4\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight4\")\n",
    "b4 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias4\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W4) + b4) # layer1 전달받음  \n",
    "\n",
    "\n",
    "# hidden layer5\n",
    "W5 = tf.Variable(tf.random_normal([50, 1]), tf.float32, name=\"weight5\")\n",
    "b5 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias5\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W5) + b5) # layer1 전달받음  \n",
    "\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    늘리긴 했지만 뭐 원래에도 정확도가 1이였기 때문에 \n",
    "    \n",
    "    결과 여전히 1로 나온다... \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://docs.google.com/presentation/d/1KHpjyziDm0Wle-OI-6TZhWM2Oj7YiypXuZOZ1SJW8ds/edit#slide=id.g1d14145db6_0_53\n",
    "    \n",
    "    여기에는 히든 레이어를 9단계 하였다.\n",
    "    \n",
    "    \n",
    "    \n",
    "    그런데 이러한 과정으로 하던 중에\n",
    "    \n",
    "    Vanishing gradient 문제가 발생하였다.   - 점점 값이 약해지는 것\n",
    "    \n",
    "    Back pro어쩌고.. 너무 deep하면 반대로 구축을 해나갈때 뒷부분에 희미해지기 시작한다.\n",
    "    \n",
    "    sigmoid를 통해 연결된 것이기 때문에 0~1사이에서는 미분되면 값이 작아지는건 당연.\n",
    "    \n",
    "    그래서 이렇게 찾는게 잘못되었다.. 시그모이드로 오차를 찾는건 거의 불가능하다.\n",
    "    \n",
    "    그래서 나온 방법이 Sigmoid의 단점을 보완하기 위해 비선형 문제에서 선형으로 변경 해버렸다.\n",
    "    \n",
    "    ->>> max 함수를 사용한다.  최소값은 완전 0으로 쓰고 최고는 주어진 값 그대로 쓴다.\n",
    "\n",
    "                        < ReLU 방식 >\n",
    "                        \n",
    "                             y\n",
    "                             |    /\n",
    "                             |   /\n",
    "                             |  /\n",
    "                             | /\n",
    "                ==============-------------> x\n",
    "                \n",
    "                \n",
    "    Sigmoid를 쓰는거보단 ReLU방식을 쓰는 게 유리하다. \n",
    "    \n",
    "    20~30단 정도는 상관없는데  숫자가 커질 수록 Sigmoid경우 정확도가 떨어질 수가 있음.\n",
    "    ReLU로 해결할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서보드 구현\n",
    "\n",
    "    https://docs.google.com/presentation/d/1KHpjyziDm0Wle-OI-6TZhWM2Oj7YiypXuZOZ1SJW8ds/edit#slide=id.g1d14145db6_0_0\n",
    "    \n",
    "    -> 18장에 구현 방식이 나와있다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.05915758]\n",
      " [0.98923934]\n",
      " [0.98889804]\n",
      " [0.97023785]\n",
      " [0.9896766 ]\n",
      " [0.9702037 ]\n",
      " [0.9701761 ]\n",
      " [0.07878938]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "####### 내가 그래프로 그리고 싶은 부분을 with로 묶는다   (여기서는 layer1로 묶었다.)\n",
    "####### 딱 처음만 보고 싶다면  이렇게 묶으면 된다.\n",
    "# hidden layer1\n",
    "with tf.name_scope(\"layer1\") :\n",
    "    W1 = tf.Variable(tf.random_normal([3, 2]), tf.float32, name=\"weight\")\n",
    "    b1 = tf.Variable(tf.random_normal([2]), tf.float32, name=\"bias\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)  \n",
    "    # 어떤 그래프로 볼 것인지 정해준다.\n",
    "    # 첫번째 단계의 가중치 값들만 모아서 보여줌\n",
    "    \n",
    "    tf.summary.histogram(\"bias1\", b1)  \n",
    "    # 히스토그램으로 bias를 보여줌\n",
    "    \n",
    "    tf.summary.histogram(\"layer1\", layer1)  \n",
    "    # 히스토그램으로 layer를 보여줌\n",
    "\n",
    "    \n",
    "# hidden layer2\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), tf.float32, name=\"weight\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) # layer1 전달받음  \n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    \n",
    "    \n",
    "    #위에서 그룹으로 묶은 것을 출력할 때\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    \n",
    "    #어디에다가 저장할건지?  전에는 log_dir에 저장했을 것이다.  여기서는 log_dir/alpha01에 저장\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "    \n",
    "    # 하나의 그래프로 그릴 수 있게 합쳐준다. 이것도 파일로 저장을 해놔야한다.\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        #tensor board를 그리기 위해 train뿐만 아니라 merged_summary를 실행시켜 줘야한다\n",
    "        _, summary = sess.run([train, merged_summary], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "        # global_step으로 몇번 실행시키는건지 알려줘야 한다. \n",
    "        writer.add_summary(summary, global_step = step) \n",
    "        \n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    저장폴더에 저장이 되었는지 확인해본다.\n",
    "    \n",
    "    그 후 cmd에서 activate tf1실행 후 \n",
    "    \n",
    "    -> 해당 위치까지 들어가서\n",
    "    -> tensorboard --logdir=./log_dir2/alpha01\n",
    "    \n",
    "    \n",
    "    우리가 그래프에서 중요하게 봐야할 것은 가중치가 어떻게 변화하는가 이다. \n",
    "    그런데 히스토그램으로 그리게 되면 사실상 보기가 어렵다. \n",
    "   \n",
    "    \n",
    "    -----------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    만약 이걸 다시 실행시키면 오류가 날 것이다.\n",
    "    \n",
    "    오류가 아니라 기존 파일에 덮어씌우지 못하는 현상인데\n",
    "    덮어씌우고 싶다면  맨 위에 reset_default_graph()를 적어준다.\n",
    "    그리고 커널을 다시 실행시켜서\n",
    "    \n",
    "    다시 실행해본다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 둘째 단, cost, 정확도를 텐서플로우에 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.03957582]\n",
      " [0.99284583]\n",
      " [0.9928026 ]\n",
      " [0.9829843 ]\n",
      " [0.99294513]\n",
      " [0.98297924]\n",
      " [0.98297703]\n",
      " [0.04411352]] \n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # 출력의 개수만 지정해주기\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "####### 내가 그래프로 그리고 싶은 부분을 with로 묶는다   (여기서는 layer1로 묶었다.)\n",
    "####### 딱 처음만 보고 싶다면  이렇게 묶으면 된다.\n",
    "# hidden layer1\n",
    "with tf.name_scope(\"layer1\") :\n",
    "    W1 = tf.Variable(tf.random_normal([3, 2]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([2]), tf.float32, name=\"bias1\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)  \n",
    "    # 어떤 그래프로 볼 것인지 정해준다.\n",
    "    # 첫번째 단계의 가중치 값들만 모아서 보여줌\n",
    "    \n",
    "    tf.summary.histogram(\"bias1\", b1)  \n",
    "    # 히스토그램으로 bias를 보여줌\n",
    "    \n",
    "    tf.summary.histogram(\"layer1\", layer1)  \n",
    "    # 히스토그램으로 layer를 보여줌\n",
    "\n",
    "    \n",
    "# hidden layer2\n",
    "with tf.name_scope(\"layer2\") :\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) # layer1 전달받음  \n",
    "\n",
    "    tf.summary.histogram(\"weight2\", W2)      \n",
    "    tf.summary.histogram(\"bias2\", b2)      \n",
    "    tf.summary.histogram(\"hoypothesis\", hypothesis)\n",
    "    \n",
    "    \n",
    "#코스트 부분 살펴보기 \n",
    "with tf.name_scope(\"cost\") :\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "        \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    #위에서 그룹으로 묶은 것을 출력할 때\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    \n",
    "    #어디에다가 저장할건지?  전에는 log_dir에 저장했을 것이다.  여기서는 log_dir/alpha01에 저장\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "    \n",
    "    # 하나의 그래프로 그릴 수 있게 합쳐준다. 이것도 파일로 저장을 해놔야한다.\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        #tensor board를 그리기 위해 train뿐만 아니라 merged_summary를 실행시켜 줘야한다\n",
    "        _, summary = sess.run([train, merged_summary], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "        # global_step으로 몇번 실행시키는건지 알려줘야 한다. \n",
    "        writer.add_summary(summary, global_step = step) \n",
    "        \n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    그래프를 확인해보면\n",
    "    \n",
    "    Cost라고 하나로 묶인것을 확인할 수 있고\n",
    "    \n",
    "    위 메뉴에 Scalar가 생긴것을 확인할 수 있다.\n",
    "    \n",
    "    들어가보면 코스트가 내려가면서 학습이 잘 됨을 알 수 있다.\n",
    "    \n",
    "    다만 학습이 0.3정도 내려간뒤로 학습이 안되고 있다.\n",
    "    \n",
    "    Wide와 deep을 조절해서 이것을 더 줄여나갈 수 있게 해야한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파라미터 값을 변경해서 비교\n",
    "\n",
    "\n",
    "    시작하기에 앞서 미리 생성된 log파일들 다 삭제해준다. \n",
    "    그리고 다른 위치로 해준다 log_dir2/alpha001 \n",
    "    위의 코드를 수정해서 learning_rate = 0.001\n",
    "    \n",
    "    그리고 위 코드를 다시 실행하고\n",
    "    밑의 코드도 실행한다. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.7736503 ]\n",
      " [0.7468454 ]\n",
      " [0.7520118 ]\n",
      " [0.7257467 ]\n",
      " [0.7779274 ]\n",
      " [0.7538905 ]\n",
      " [0.7478278 ]\n",
      " [0.72489256]] \n",
      "예측 :  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 :  0.75\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "# hidden layer1\n",
    "with tf.name_scope(\"layer1\") :\n",
    "    W1 = tf.Variable(tf.random_normal([3, 2]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([2]), tf.float32, name=\"bias1\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)   \n",
    "    tf.summary.histogram(\"bias1\", b1)   \n",
    "    tf.summary.histogram(\"layer1\", layer1)  \n",
    "\n",
    "    \n",
    "# hidden layer2\n",
    "with tf.name_scope(\"layer2\") :\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2) \n",
    "\n",
    "    tf.summary.histogram(\"weight2\", W2)      \n",
    "    tf.summary.histogram(\"bias2\", b2)      \n",
    "    tf.summary.histogram(\"hoypothesis\", hypothesis)\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"cost\") :\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "        \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        _, summary = sess.run([train, merged_summary], feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step = step) \n",
    "        \n",
    "        \n",
    "    h, c, a = sess.run([hypothesis, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "    print(\"가설 : \", h, \"\\n예측 : \", c, \"\\n정확도 : \", a)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    실행 시키고 나면\n",
    "    \n",
    "    두가지 폴더에 두개의 log가 생성되었음을 알 수 있다.\n",
    "    \n",
    "    이걸 텐서플로우에 그리려면 따로따로 실행하지 말고 \n",
    "    \n",
    "    동시에 상위폴더인 log_dir2를 실행해보면 alpha01, alpha001이 다 실행된다.\n",
    "    \n",
    "    tensorboard --logdir=./log_dir2\n",
    "    \n",
    "    \n",
    "#### 그래프 결과\n",
    "    Scalar 메뉴에서는\n",
    "    \n",
    "    정확도와 cost 값을 볼 수 있다.\n",
    "    \n",
    "    정확도의 경우\n",
    "        - 파란색이 0.001일 경우인데 수치가 더 안좋음을 알 수 있다.\n",
    "    \n",
    "    cost의 경우\n",
    "        - 빨간색은 최저를 향해 가므로 아주 좋아지고 있다고 할 수 있지만\n",
    "          파란색은 내려가지를 않고 있으므로 학습이 안되고 있음을 알 수 있다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래서 Vanishing Gradient의 해결은?\n",
    "\n",
    "    손글씨 이미지 샘플로 Back pr성능을 나아지게 하는 방법\n",
    "    \n",
    "    ReLU를 쓴다.\n",
    "    \n",
    "    \n",
    "    http://hunkim.github.io/ml/lec10.pdf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-26b94c9572db>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000017F22594BC8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000017F22A2DF48>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000017F2294B548>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])   #28행 28열이므로\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28*28, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logit = tf.matmul(X, W) + b\n",
    "\n",
    "# 원래 이진분류에선 Sigmoid를 사용했는데 다중 분류이므로 softmax\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels = y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 2.946887087171731\n",
      "epoch : 2 cost : 1.0820250979336823\n",
      "epoch : 3 cost : 0.8525083774870093\n",
      "epoch : 4 cost : 0.7568003275177697\n",
      "epoch : 5 cost : 0.6833747980811378\n",
      "epoch : 6 cost : 0.6373064898360863\n",
      "epoch : 7 cost : 0.6005993575941435\n",
      "epoch : 8 cost : 0.5710864874449643\n",
      "epoch : 9 cost : 0.545734549002214\n",
      "epoch : 10 cost : 0.5307045804370533\n",
      "epoch : 11 cost : 0.5127253646200355\n",
      "epoch : 12 cost : 0.4975546587055381\n",
      "epoch : 13 cost : 0.48481160413135194\n",
      "epoch : 14 cost : 0.47196521444754175\n",
      "epoch : 15 cost : 0.46128555471246874\n",
      "epoch : 16 cost : 0.4541235127774152\n",
      "epoch : 17 cost : 0.44421487136320614\n",
      "epoch : 18 cost : 0.43559210202910686\n",
      "epoch : 19 cost : 0.42844437301158933\n",
      "epoch : 20 cost : 0.4244581237164412\n",
      "epoch : 21 cost : 0.41810212438756783\n",
      "epoch : 22 cost : 0.40734511088241215\n",
      "epoch : 23 cost : 0.40434401571750656\n",
      "epoch : 24 cost : 0.40125996654683915\n",
      "epoch : 25 cost : 0.3936130424521188\n",
      "epoch : 26 cost : 0.3903421963886784\n",
      "epoch : 27 cost : 0.3873243796825408\n",
      "epoch : 28 cost : 0.3813078879226336\n",
      "epoch : 29 cost : 0.3800086318362842\n",
      "epoch : 30 cost : 0.37171925300901604\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    cost값이 점점 줄어드는 것을 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9005\n"
     ]
    }
   ],
   "source": [
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    얘는 정확도가 90%가 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이어 3개 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 2.304516487988558\n",
      "epoch : 2 cost : 2.278695405613293\n",
      "epoch : 3 cost : 1.901167440414429\n",
      "epoch : 4 cost : 1.6117805060473354\n",
      "epoch : 5 cost : 1.4189706364544954\n",
      "epoch : 6 cost : 1.3044782088019629\n",
      "epoch : 7 cost : 1.2336401852694416\n",
      "epoch : 8 cost : 1.1932732564752748\n",
      "epoch : 9 cost : 1.1602070082317693\n",
      "epoch : 10 cost : 1.128934054157951\n",
      "epoch : 11 cost : 1.112649618062106\n",
      "epoch : 12 cost : 1.0918966039744298\n",
      "epoch : 13 cost : 1.073668015003204\n",
      "epoch : 14 cost : 1.0612193571437494\n",
      "epoch : 15 cost : 1.0562941867654971\n",
      "epoch : 16 cost : 1.0390430430932482\n",
      "epoch : 17 cost : 1.0223956517739734\n",
      "epoch : 18 cost : 1.007840049917047\n",
      "epoch : 19 cost : 0.9897912311553965\n",
      "epoch : 20 cost : 0.9807121008092715\n",
      "epoch : 21 cost : 0.9663441107489849\n",
      "epoch : 22 cost : 0.9495773159373887\n",
      "epoch : 23 cost : 0.9266835288567975\n",
      "epoch : 24 cost : 0.9143834003535182\n",
      "epoch : 25 cost : 0.8989269681410361\n",
      "epoch : 26 cost : 0.8879952565106475\n",
      "epoch : 27 cost : 0.8742260052941067\n",
      "epoch : 28 cost : 0.8642041058973831\n",
      "epoch : 29 cost : 0.8518962933800436\n",
      "epoch : 30 cost : 0.8379914205724543\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])   #28행 28열이므로\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.softmax(logit)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.softmax(logit)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.softmax(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels = y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost)\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.6444\n"
     ]
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    엄청 deep하게 해봤더니 정확도가 급격히 낮아졌다.\n",
    "    \n",
    "    무엇을 의미할까? 중간에서 판단을 잘 못해주고 있기 때문이다.\n",
    "    \n",
    "    softmax -> sigmoid로  변경하면\n",
    "    \n",
    "    즉 activation_function 을 바꿔주면 성능이 올라간다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 3.3011024250767425\n",
      "epoch : 2 cost : 0.5364300460707054\n",
      "epoch : 3 cost : 0.36077780425548567\n",
      "epoch : 4 cost : 0.29263955243609163\n",
      "epoch : 5 cost : 0.2401800020716408\n",
      "epoch : 6 cost : 0.20716553303328417\n",
      "epoch : 7 cost : 0.18181633737954225\n",
      "epoch : 8 cost : 0.1574164982410994\n",
      "epoch : 9 cost : 0.1387524758550254\n",
      "epoch : 10 cost : 0.12499541846188633\n",
      "epoch : 11 cost : 0.1118015777929263\n",
      "epoch : 12 cost : 0.0994433491067452\n",
      "epoch : 13 cost : 0.0885569567707451\n",
      "epoch : 14 cost : 0.07994735698131003\n",
      "epoch : 15 cost : 0.07211035035550592\n",
      "epoch : 16 cost : 0.06522634207525035\n",
      "epoch : 17 cost : 0.057783458022908706\n",
      "epoch : 18 cost : 0.05354666816917331\n",
      "epoch : 19 cost : 0.047214175638827426\n",
      "epoch : 20 cost : 0.043797805380414806\n",
      "epoch : 21 cost : 0.0393105594339696\n",
      "epoch : 22 cost : 0.036191525472836064\n",
      "epoch : 23 cost : 0.032021325481208854\n",
      "epoch : 24 cost : 0.030406750166280704\n",
      "epoch : 25 cost : 0.02773963933979924\n",
      "epoch : 26 cost : 0.025296720594845022\n",
      "epoch : 27 cost : 0.023658465668559073\n",
      "epoch : 28 cost : 0.021574701729484582\n",
      "epoch : 29 cost : 0.020116692893207066\n",
      "epoch : 30 cost : 0.018778334608809528\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])   #28행 28열이므로\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.sigmoid(logit)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.sigmoid(logit)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.sigmoid(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.sigmoid(logit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels = y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.8).minimize(cost)\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.935\n"
     ]
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    충분히 좋아졌다.\n",
    "    그런데 sigmoid -> ReLU로 변경하면 성능이 더 좋아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : nan\n",
      "epoch : 2 cost : nan\n",
      "epoch : 3 cost : nan\n",
      "epoch : 4 cost : nan\n",
      "epoch : 5 cost : nan\n",
      "epoch : 6 cost : nan\n",
      "epoch : 7 cost : nan\n",
      "epoch : 8 cost : nan\n",
      "epoch : 9 cost : nan\n",
      "epoch : 10 cost : nan\n",
      "epoch : 11 cost : nan\n",
      "epoch : 12 cost : nan\n",
      "epoch : 13 cost : nan\n",
      "epoch : 14 cost : nan\n",
      "epoch : 15 cost : nan\n",
      "epoch : 16 cost : nan\n",
      "epoch : 17 cost : nan\n",
      "epoch : 18 cost : nan\n",
      "epoch : 19 cost : nan\n",
      "epoch : 20 cost : nan\n",
      "epoch : 21 cost : nan\n",
      "epoch : 22 cost : nan\n",
      "epoch : 23 cost : nan\n",
      "epoch : 24 cost : nan\n",
      "epoch : 25 cost : nan\n",
      "epoch : 26 cost : nan\n",
      "epoch : 27 cost : nan\n",
      "epoch : 28 cost : nan\n",
      "epoch : 29 cost : nan\n",
      "epoch : 30 cost : nan\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])   #28행 28열이므로\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels = y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost)\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9273\n"
     ]
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ???? cost범위가 초월했다. \n",
    "    \n",
    "    이 상황에서는 sigmoid가 낫다고 생각하자.\n",
    "    \n",
    "    더 좋게 하려면\n",
    "    \n",
    "    초기값이 중요하다고 한다.   \n",
    "    \n",
    "    \n",
    "    - 초기값 설정 방식 -\n",
    "    RBM 알고리즘 \n",
    "    \n",
    "        두개씩 짝지어서 가중치를 좋은것이 무엇인지 계산.\n",
    "        \n",
    "        좋긴한데 너무 복잡하다. 실용해서 쓰기 어렵다. \n",
    "        \n",
    "    \n",
    "    Xavier 초기화\n",
    "        그래서 나온 대안 알고리즘. \n",
    "        랜덤하게 값을 주긴하지만 현재 랜덤값과, 출력은 입력의 제곱값. 계산으로만 좋은 초기값을 구할 수 있게 한다.\n",
    "        \n",
    "        He라는 사람은 /2 를 했더니 더 좋아지더라 \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-21a1d2ddfa3d>:33: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "epoch : 1 cost : 0.6765587370504034\n",
      "epoch : 2 cost : 0.2714642259749498\n",
      "epoch : 3 cost : 0.209924691780047\n",
      "epoch : 4 cost : 0.16881890814412728\n",
      "epoch : 5 cost : 0.1418534609946338\n",
      "epoch : 6 cost : 0.12126039553772316\n",
      "epoch : 7 cost : 0.10471004326235166\n",
      "epoch : 8 cost : 0.09221948606046768\n",
      "epoch : 9 cost : 0.081183653738011\n",
      "epoch : 10 cost : 0.07309369143437253\n",
      "epoch : 11 cost : 0.06507673322477125\n",
      "epoch : 12 cost : 0.058235729373991504\n",
      "epoch : 13 cost : 0.05297029497948559\n",
      "epoch : 14 cost : 0.04807007345963607\n",
      "epoch : 15 cost : 0.042415450560775655\n",
      "epoch : 16 cost : 0.038523191010410136\n",
      "epoch : 17 cost : 0.03402271817692303\n",
      "epoch : 18 cost : 0.03106901339170605\n",
      "epoch : 19 cost : 0.02798845845867287\n",
      "epoch : 20 cost : 0.0252377090450715\n",
      "epoch : 21 cost : 0.022764006028459814\n",
      "epoch : 22 cost : 0.020270087586885133\n",
      "epoch : 23 cost : 0.01778747664256531\n",
      "epoch : 24 cost : 0.016119235965677284\n",
      "epoch : 25 cost : 0.014839508400552655\n",
      "epoch : 26 cost : 0.012642936068163687\n",
      "epoch : 27 cost : 0.011470190595666115\n",
      "epoch : 28 cost : 0.009920937463810493\n",
      "epoch : 29 cost : 0.009220708794891825\n",
      "epoch : 30 cost : 0.008030647136355668\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#Xavier  초기화 방식\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])   #28행 28열이므로\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "# 초기값을 랜덤으로 하지 않는다.\n",
    "\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())  # 이름지정, 크기지정\n",
    "# initializer : 초기값을 어떤 방식을 사용할건지 지정\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels = y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9787\n"
     ]
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    와우.. 엄청난 정확도가 나왔다.\n",
    "    이렇게 Xavier 초기값을 사용하게 되면 \n",
    "    \n",
    "    epoch1 처럼 처음부터 좋은 가중치를 잡아서 빠르게 정확도를 맞출수있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 좀 더 deep하고 wide하게 만들어보기\n",
    "    deep : 8 layer까지\n",
    "    wide : 512로 올려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 1.800648354833775\n",
      "epoch : 2 cost : 0.45202006968584924\n",
      "epoch : 3 cost : 0.263416669720953\n",
      "epoch : 4 cost : 0.20674541118470105\n",
      "epoch : 5 cost : 0.19023984933441315\n",
      "epoch : 6 cost : 0.14048406401818436\n",
      "epoch : 7 cost : 0.11950595274567609\n",
      "epoch : 8 cost : 0.11072531794959847\n",
      "epoch : 9 cost : 0.08762073914435778\n",
      "epoch : 10 cost : 0.07693952450020747\n",
      "epoch : 11 cost : 0.06783604955131355\n",
      "epoch : 12 cost : 0.061524695828557005\n",
      "epoch : 13 cost : 0.0518330900015479\n",
      "epoch : 14 cost : 0.046065156646072865\n",
      "epoch : 15 cost : 0.3318271639164199\n",
      "epoch : 16 cost : 0.07038502331145786\n",
      "epoch : 17 cost : 0.044996406969360345\n",
      "epoch : 18 cost : 0.03684467492455791\n",
      "epoch : 19 cost : 0.41548533822155825\n",
      "epoch : 20 cost : 0.06711404184726154\n",
      "epoch : 21 cost : 0.040540037720718146\n",
      "epoch : 22 cost : 0.02878449598581278\n",
      "epoch : 23 cost : 0.11967750659254807\n",
      "epoch : 24 cost : 0.02367903303100982\n",
      "epoch : 25 cost : 0.016056043058422127\n",
      "epoch : 26 cost : 0.012233994317833669\n",
      "epoch : 27 cost : 0.009644013101192703\n",
      "epoch : 28 cost : 0.007265046491626309\n",
      "epoch : 29 cost : 0.00587554428895766\n",
      "epoch : 30 cost : 0.005247321779712697\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#Xavier  초기화 방식\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])   #28행 28열이므로\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "# 초기값을 랜덤으로 하지 않는다.\n",
    "\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())  # 이름지정, 크기지정\n",
    "# initializer : 초기값을 어떤 방식을 사용할건지 지정\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.relu(logit)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels = y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9784\n"
     ]
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    완전 서능차이가 나진 않는다. \n",
    "    \n",
    "    deep 을 높이고 wide도 높였는데 \n",
    "    \n",
    "    왜 성능차이가 없을까 \n",
    "    \n",
    "    \n",
    "    훈련데이터에서 너무 과하게 한 즉.. 과적합 문제이다. \n",
    "    \n",
    "    \n",
    "    그러면 어떻게 튜닝해야할까?\n",
    "    \n",
    "    튜닝방법 \n",
    "        - More training data              \n",
    "    \n",
    "        - reduce the number of features   : feature의 갯수 조정\n",
    "    \n",
    "        - regularization    : L2 norm, L1 norm  의 조정\n",
    "        \n",
    "                              Dropout : 뉴럴네트워크도 너무 여러개 노드기 때문에 과적합이니까 \n",
    "                                        노드 몇개 좀 빼서 계산하자. (훈련할 때만 빼고, 실제 실행시는 돌림)\n",
    "                                        \n",
    "                              \n",
    "    * Drop out \n",
    "        tf.nn.dropout(_L1, dropout_rate)  \n",
    "            dropout_rate : 몇퍼센트 빼고 할건지 \n",
    "            그래서 dropout_rate는 placeholder로 이용한다.  훈련시에만 이용할 것이기 때문에\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위 코드가 과적합인거 같아서 처리하는 방법\n",
    "\n",
    "#### drop out 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 cost : 2.33165286627683\n",
      "epoch : 2 cost : 1.4078201005675584\n",
      "epoch : 3 cost : 0.6734717642177239\n",
      "epoch : 4 cost : 0.46320600379597066\n",
      "epoch : 5 cost : 0.3682170871171086\n",
      "epoch : 6 cost : 0.3129105698520489\n",
      "epoch : 7 cost : 0.2717283888838507\n",
      "epoch : 8 cost : 0.2479287718913772\n",
      "epoch : 9 cost : 0.2238542670282451\n",
      "epoch : 10 cost : 0.20283503212712053\n",
      "epoch : 11 cost : 0.18898581193252048\n",
      "epoch : 12 cost : 0.1781947169791568\n",
      "epoch : 13 cost : 0.16371400537815956\n",
      "epoch : 14 cost : 0.15244001142003322\n",
      "epoch : 15 cost : 0.14782311992211775\n",
      "epoch : 16 cost : 0.1385484650595622\n",
      "epoch : 17 cost : 0.13383529861542323\n",
      "epoch : 18 cost : 0.12643660829825837\n",
      "epoch : 19 cost : 0.1199031442268328\n",
      "epoch : 20 cost : 0.11343215328048584\n",
      "epoch : 21 cost : 0.11291038805110883\n",
      "epoch : 22 cost : 0.10614662160250268\n",
      "epoch : 23 cost : 0.10302142042328015\n",
      "epoch : 24 cost : 0.0986257614059882\n",
      "epoch : 25 cost : 0.09563353885981164\n",
      "epoch : 26 cost : 0.09188330285251138\n",
      "epoch : 27 cost : 0.08961109773679213\n",
      "epoch : 28 cost : 0.08481372201307252\n",
      "epoch : 29 cost : 0.08528093739666726\n",
      "epoch : 30 cost : 0.08119620475579391\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#Xavier  초기화 방식\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])   #28행 28열이므로\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "# 초기값을 랜덤으로 하지 않는다.\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())  # 이름지정, 크기지정\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=prob)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=prob)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "layer3 = tf.nn.dropout(layer3, keep_prob=prob)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "layer4 = tf.nn.dropout(layer4, keep_prob=prob)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.relu(logit)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "layer5 = tf.nn.dropout(layer5, keep_prob=prob)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "layer6 = tf.nn.dropout(layer6, keep_prob=prob)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.nn.relu(logit)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "layer7 = tf.nn.dropout(layer7, keep_prob=prob)  #keep_prob : 얼마나 남기고 훈련시킬지 결정\n",
    "\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.relu(logit)\n",
    "hypothesis = tf.nn.dropout(hypothesis, keep_prob=prob)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels = y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수 준비\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs) :\n",
    "    fetal_batch = int(mnist.train.num_examples/batch_size) #mnist.train.num_examples 전체데이터 55000개를 batch_size 만큼 나누기\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(fetal_batch):\n",
    "        batch_ws, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_ws, y:batch_ys, prob:0.7})\n",
    "        avg_cost += c/fetal_batch\n",
    "        \n",
    "    print(\"epoch :\", (epoch+1), \"cost :\", avg_cost)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9792\n"
     ]
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    feed_dict을 보면 prob가 1이 주어졌다.  아까 위에서 훈련할 땐 0.7 즉 70%를 했지만\n",
    "    \n",
    "    실제 데이터에서는 100%로 해야하기 때문이다. \n",
    "    \n",
    "    정확도가 어느정도 올라갔다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    우리는 grdient descent 어쩌고를 쓰고 있는데 이게 기능이 안좋다면 \n",
    "\n",
    "    아담 옵티마이저 를 바꿔보는것도 나쁘진 않다. \n",
    "\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "        아담은 learning rate를 낮춰서 하는게 좋다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    feed forward -> 우리가 한 일반적인 방식 \n",
    "    \n",
    "    ensemble -> 여러개를 돌려서 가장 좋은것을 취합하는 방식 (투표하는 방식 voting )\n",
    "        softvoting\n",
    "        hardvoting\n",
    "        \n",
    "    \n",
    "    RNN -> 여러가지 방식을 계속 나눠서 가능 방식\n",
    "    CNN -> 여러개로 쪼개서 하나로 합치는 방식 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN  굿 \n",
    "\n",
    "    - 이미지 구별에 가장 많이 사용된다. \n",
    "    - 요즘에는 자연어 처리에서 많이 쓰인다. \n",
    "\n",
    "    - 고양이 그림 아주 유명하죠\n",
    "        고양이들이 학습할 때 하나로 인식하는게 아니라 따로따로 인식한 뒤에 조합하더라 .  여기서 나온 방식 \n",
    "    \n",
    "    - 그림을 쪼개서 특징을 뽑는 방식\n",
    "    \n",
    "    \n",
    "        stride : 이동 칸 수 \n",
    "            1  (1칸씩이동), 2(두칸씩이동)\n",
    "        \n",
    "        padding\n",
    "        \n",
    "    \n",
    "    컨볼루션 \n",
    "    - 이미지크기에서 filter의 크기가 몇일 때 몇개의 이미지가 나오는가?\n",
    "    \n",
    "        (N-F/stride) +1  이것이 정수로 나누어지않는다면 스트라이브가 잘못되었을것\n",
    "        \n",
    "    \n",
    "    -   padding \n",
    "        보통0을 채워준다.  그래서 경계선을 그릴수도 있고 이미지크기를 보존할수있는 장점이있다. \n",
    "                                                                                                                                                          \n",
    "                                                                                                                                              - 필터를 보통 하나만 쓰는 경우는 없다.\n",
    "                                                                                                                                                  또다른 필터를 준다는것은 또다른 가중치로 뽑아내서 새로운 이미지를 만듬\n",
    "                                                                                                                                           \n",
    "                                                                                                                                               머신러닝은 feature의 특징을 정하는것이 핵심\n",
    "                                                                                                                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Max pooling\n",
    "    \n",
    "    원본이미지를 필터로 잘라서 나온 새로운 이미지중\n",
    "    \n",
    "    거기서 가장 큰 값을 해줌\n",
    "    \n",
    "    적절히 이용하면 아주 좋다.\n",
    "    \n",
    "    -----------------여기까지가 CNN 전처리 작업이다. --------------------------\n",
    "    \n",
    "    이제 이것을 인공신경망에다가 넘겨줘서 (Fully Connected Layer)\n",
    "    \n",
    "    여기서 이게 자동차인지 새인지 결과를 뽑아주는 역할을 함\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
